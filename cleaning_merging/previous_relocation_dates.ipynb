{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ignore 'dask' warning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gcsfs\n",
    "from google.cloud import storage\n",
    "from pandas import DataFrame\n",
    "from IPython.display import HTML\n",
    "from google.cloud.storage import Blob\n",
    "import datalab.storage as gcs_datalab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up constants\n",
    "\n",
    "All required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project = 'graydon-moving-indicator'\n",
    "bucket_name = 'graydon-data'\n",
    "dir_year_files_from = '02_cleaned'\n",
    "dir_year_files_to = 'files_to_bucket'\n",
    "years = ['2018']\n",
    "selected_columns_small = ['date_month', 'id_company', 'id_branch', 'date_start', 'from_date_start']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(project='graydon-moving-indicator')\n",
    "gcs = storage.Client()\n",
    "bucket = gcs.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_one_year_from_bucket_merged_csv(year, dir_prefix = '', selected_columns = ''):\n",
    "    \"\"\" Reads a whole year of data from the already merged files \"\"\"\n",
    "    full_year_df = pd.DataFrame()\n",
    "    \n",
    "    # Setting up dictionary of column types\n",
    "    dtype={'id_company'  :np.float64,\n",
    "           'id_branch'    :np.int64,\n",
    "           'is_discontinued':bool,\n",
    "           'code_discontinuation': np.float64,\n",
    "           'code_financial_calamity':object,\n",
    "           'financial_calamity_outcome'   : np.float64,\n",
    "           'code_legal_form' : np.float64,\n",
    "           #'qty_employees' :np.float64,\n",
    "           #'year_qty_employees' :np.float64,\n",
    "           'id_company_creditproxy':object,\n",
    "           'score_payment_assessment'    : np.float64,\n",
    "           #'amt_revenue'  : np.float64,\n",
    "           'year_revenue'  : np.float64,\n",
    "           #'amt_operating_result'   : np.float64,\n",
    "           #'year_operating_result'    :object,\n",
    "           #'amt_consolidated_revenue'   : np.float64,\n",
    "           #'year_consolidated_revenue'   :object,\n",
    "           #'amt_consolidated_operating_result'     : np.float64,\n",
    "           #'year_consolidated_operating_result'   :object,\n",
    "           'qty_issued_credit_reports' : np.float64,\n",
    "           'perc_credit_limit_adjustment' :object,\n",
    "           'color_credit_status'  :object,\n",
    "           'rat_pd'              :object,\n",
    "           #'score_pd'            : np.float64,\n",
    "           'has_increased_risk'  :bool,\n",
    "           'is_sole_proprietor'   :bool,\n",
    "           'code_sbi_2'         : np.float64,\n",
    "           'code_sbi_1'          :object,\n",
    "           'qty_address_mutations_total'  :np.float64,\n",
    "           'qty_address_mutations_month'   :np.float64,\n",
    "           'has_relocated':bool,\n",
    "           'qty_started_names': np.float64,\n",
    "           'qty_stopped_names': np.float64,\n",
    "           'has_name_change':bool,\n",
    "           'total_changeof_board_members_' :np.float64\n",
    "         }\n",
    "    \n",
    "    parse_dates= ['date_established' ,'date_established', 'date_financial_calamity_started',\n",
    "           'date_financial_calamity_stopped', 'date_start', 'from_date_start' ]\n",
    "    \n",
    "    blob_list = list(bucket.list_blobs(prefix=dir_prefix))    \n",
    "    for blob in blob_list:  \n",
    "        if year in blob.name:\n",
    "            with fs.open('graydon-data/' + blob.name) as f:\n",
    "                if selected_columns == '' or None:\n",
    "                    full_year_df = pd.read_csv(f, sep=',', index_col=0, dtype=dtype, parse_dates=parse_dates) \n",
    "                else:\n",
    "                    full_year_df = pd.read_csv(f, sep=',', index_col=0, \n",
    "                                               usecols = selected_columns, dtype=dtype, parse_dates=parse_dates) \n",
    "            print('The number of rows read: ', full_year_df.shape[0])\n",
    "    return full_year_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_combined_years(year, dir_year_files, selected_columns = None):\n",
    "\n",
    "    df_all_years = pd.DataFrame()\n",
    "\n",
    "    for year in years:\n",
    "        df_one_year = read_one_year_from_bucket_merged_csv(year = year, \n",
    "                                                          dir_prefix = dir_year_files,\n",
    "                                                          selected_columns = selected_columns)\n",
    "        df_all_years = df_all_years.append(df_one_year)\n",
    "\n",
    "    return df_all_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_locally(df, dir_prefix, year, as_json= False):\n",
    "    \"\"\" Saves df as json or csv locally on server \"\"\"\n",
    "    if as_json:        \n",
    "        file_path = dir_prefix + '/' + year + '_merged.json'\n",
    "        df.to_json(file_path)\n",
    "    else:\n",
    "        file_path =  dir_prefix + '/' + year + '_merged.csv'\n",
    "        df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading relocation dates\n",
    "blob_list = list(bucket.list_blobs(prefix='01_input/additional_data/location_start_date.CSV'))\n",
    "\n",
    "for blob in blob_list: \n",
    "    with fs.open('graydon-data/' + blob.name) as f:\n",
    "        df_relocation_dates = pd.read_csv(f, sep=',', \n",
    "                                          na_values=['', '1198-06-12', 'NA']) \n",
    "        df_relocation_dates['date_relocation_last'] = pd.to_datetime(df_relocation_dates['date_relocation_last'])\n",
    "        df_relocation_dates['date_relocation_penultimate'] = pd.to_datetime(df_relocation_dates['date_relocation_penultimate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_company</th>\n",
       "      <th>id_branch</th>\n",
       "      <th>date_relocation_last</th>\n",
       "      <th>date_relocation_penultimate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>10079408</td>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>10079408</td>\n",
       "      <td>2015-06-11</td>\n",
       "      <td>2014-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>29898765</td>\n",
       "      <td>1998-02-21</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>10079424</td>\n",
       "      <td>2001-06-15</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>10079432</td>\n",
       "      <td>1997-03-20</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview of the data \n",
    "HTML(DataFrame(df_relocation_dates).head(5).to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_year = '2018'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove this stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \"\"\" Reads a whole year of data from the already merged files \"\"\"\n",
    "    full_year_df = pd.DataFrame()\n",
    "    \n",
    "    # Setting up dictionary of column types\n",
    "    dtype={'id_company'  :np.float64,\n",
    "           'id_branch'    :np.int64,\n",
    "           'is_discontinued':bool,\n",
    "           'code_discontinuation': np.float64,\n",
    "           'code_financial_calamity':object,\n",
    "           'financial_calamity_outcome'   : np.float64,\n",
    "           'code_legal_form' : np.float64,\n",
    "           #'qty_employees' :np.float64,\n",
    "           #'year_qty_employees' :np.float64,\n",
    "           'id_company_creditproxy':object,\n",
    "           'score_payment_assessment'    : np.float64,\n",
    "           #'amt_revenue'  : np.float64,\n",
    "           'year_revenue'  : np.float64,\n",
    "           #'amt_operating_result'   : np.float64,\n",
    "           #'year_operating_result'    :object,\n",
    "           #'amt_consolidated_revenue'   : np.float64,\n",
    "           #'year_consolidated_revenue'   :object,\n",
    "           #'amt_consolidated_operating_result'     : np.float64,\n",
    "           #'year_consolidated_operating_result'   :object,\n",
    "           'qty_issued_credit_reports' : np.float64,\n",
    "           'perc_credit_limit_adjustment' :object,\n",
    "           'color_credit_status'  :object,\n",
    "           'rat_pd'              :object,\n",
    "           #'score_pd'            : np.float64,\n",
    "           'has_increased_risk'  :bool,\n",
    "           'is_sole_proprietor'   :bool,\n",
    "           'code_sbi_2'         : np.float64,\n",
    "           'code_sbi_1'          :object,\n",
    "           'qty_address_mutations_total'  :np.float64,\n",
    "           'qty_address_mutations_month'   :np.float64,\n",
    "           'has_relocated':bool,\n",
    "           'qty_started_names': np.float64,\n",
    "           'qty_stopped_names': np.float64,\n",
    "           'has_name_change':bool,\n",
    "           'total_changeof_board_members_' :np.float64\n",
    "         }\n",
    "    \n",
    "    parse_dates= ['date_established' ,'date_established', 'date_financial_calamity_started',\n",
    "           'date_financial_calamity_stopped', 'date_start', 'from_date_start' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_year_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02_cleaned\n"
     ]
    }
   ],
   "source": [
    "print(dir_year_files_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# End remove this stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows read:  23224148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (41,42,46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows read:  22729762\n"
     ]
    }
   ],
   "source": [
    "df_year = read_one_year_from_bucket_merged_csv(year = i_year, dir_prefix = dir_year_files_from)\n",
    "qty_rows_input = len(df_year) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22729762"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qty_rows_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use only the relocations of i_year and the years before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_same_or_before_year = df_relocation_dates['date_relocation_last'].dt.year <= int(i_year)\n",
    "df_relocation_dates_year = df_relocation_dates[is_same_or_before_year]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding relocation dates for each branch month combination before the maximum date before the month date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_branch_months = df_year[['id_company', 'id_branch', 'date_month']]\n",
    "df_branch_months = df_branch_months.merge(df_relocation_dates_year, \n",
    "                                          on=['id_company', 'id_branch'], \n",
    "                                          how='left')\n",
    "df_max_dates = df_branch_months.groupby(['id_company', \n",
    "                                         'id_branch', \n",
    "                                         'date_month'])['date_relocation_last', 'date_relocation_penultimate'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the new data to the original year data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_year = df_year.merge(df_max_dates,\n",
    "                        on=['id_company', 'id_branch', 'date_month'], \n",
    "                        how='left')\n",
    "qty_rows_output = len(df_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether number of rows yearly should remain constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(qty_rows_input != qty_rows_output):\n",
    "    print('Mismatch in rows for ', i_year)\n",
    "    print(\"Going in : \", qty_rows_input)\n",
    "    print(\"Going out: \", qty_rows_output)\n",
    "    raise Exception('Mismatch in rows for: {}'.format(i_year))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_df_locally(df= df_year, dir_prefix= dir_year_files_to, year=i_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(df_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In loop form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Reading monthly branch data of  2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows read:  20764754\n",
      "2. Selecting relocation data of  2013  and before\n",
      "3. Getting relocation data for each month  2013  per branch\n",
      "4. Adding relocation dates to company data of  2013\n",
      "5. Saving enriched yearly data of  2013  locally\n",
      "6. Done processing and saving 2013\n"
     ]
    }
   ],
   "source": [
    "for i_year in years:\n",
    "    \n",
    "    # Reading year\n",
    "    print('1. Reading monthly branch data of ', i_year)\n",
    "    df_year = read_one_year_from_bucket_merged_csv(year = i_year, dir_prefix = dir_year_files_from)\n",
    "    qty_rows_input = len(df_year) \n",
    "    \n",
    "    # Use only the relocations of i_year and the years before\n",
    "    print('2. Selecting relocation data of ', i_year, \" and before\")\n",
    "    is_same_or_before_year = df_relocation_dates['date_relocation_last'].dt.year <= int(i_year)\n",
    "    df_relocation_dates_year = df_relocation_dates[is_same_or_before_year]\n",
    "    \n",
    "    # Getting relocation dates for each branch month combination before the maximum date before the month date\n",
    "    print('3. Getting relocation data for each month ', i_year, \" per branch\")\n",
    "    df_branch_months = df_year[['id_company', 'id_branch', 'date_month']]\n",
    "    df_branch_months = df_branch_months.merge(df_relocation_dates_year, \n",
    "                                              on=['id_company', 'id_branch'], \n",
    "                                              how='left')\n",
    "    df_max_dates = df_branch_months.groupby(['id_company', 'id_branch', 'date_month'])['date_relocation_last', 'date_relocation_penultimate'].max()\n",
    "    \n",
    "    # Adding the new data to the original year data\n",
    "    print('4. Adding relocation dates to company data of ', i_year)\n",
    "    df_year = df_year.merge(df_max_dates,\n",
    "                            on=['id_company', 'id_branch', 'date_month'], \n",
    "                            how='left')\n",
    "    qty_rows_output = len(df_year)\n",
    "    \n",
    "    # Check whether number of rows yearly should remain constant\n",
    "    if(qty_rows_input != qty_rows_output):\n",
    "        print('Mismatch in rows for ', i_year)\n",
    "        print(\"Going in : \", qty_rows_input)\n",
    "        print(\"Going out: \", qty_rows_output)\n",
    "        raise Exception('Mismatch in rows for: {}'.format(i_year))\n",
    "        \n",
    "    print('5. Saving enriched yearly data of ', i_year, ' locally' )       \n",
    "    save_df_locally(df= df_year, dir_prefix= dir_year_files_to, year= i_year)\n",
    "    \n",
    "    print('6. Done processing and saving', i_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graydon-moving",
   "language": "python",
   "name": "graydon-moving"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
