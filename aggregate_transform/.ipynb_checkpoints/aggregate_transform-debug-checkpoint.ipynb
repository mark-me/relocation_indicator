{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ignore 'dask' warning\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "from google.cloud import storage\n",
    "from pandas import DataFrame\n",
    "from IPython.display import HTML\n",
    "from google.cloud.storage import Blob\n",
    "import datalab.storage as gcs_datalab\n",
    "from datetime import date\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up constants. All required\n",
    "project = 'graydon-moving-indicator'\n",
    "bucket_name = 'graydon-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing bucket\n",
    "fs = gcsfs.GCSFileSystem(project='graydon-moving-indicator')\n",
    "gcs = storage.Client()\n",
    "bucket = gcs.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-aggregation\n",
    "def create_dict_types_original_data():\n",
    "    # Setting up dictionary of column types\n",
    "    dtype={ 'id_company'  :np.float64,\n",
    "        'id_branch'    :np.int64,\n",
    "        'is_discontinued':bool,\n",
    "        'code_discontinuation': np.float64,\n",
    "        'code_financial_calamity':object,\n",
    "        'financial_calamity_outcome'   : np.float64,\n",
    "        'code_legal_form' : np.float64,\n",
    "        'qty_employees' :np.float64,\n",
    "        'year_qty_employees' :np.float64,\n",
    "        'id_company_creditproxy':object,\n",
    "        'score_payment_assessment'    : np.float64,\n",
    "        'amt_revenue'  : np.float64,\n",
    "        'year_revenue'  : np.float64,\n",
    "        'amt_operating_result'   : np.float64,\n",
    "        'year_operating_result'    :object,\n",
    "        'amt_consolidated_revenue'   : np.float64,\n",
    "        'year_consolidated_revenue'   :object,\n",
    "        'amt_consolidated_operating_result'     : np.float64,\n",
    "        'year_consolidated_operating_result'   :object,\n",
    "        'qty_issued_credit_reports' : np.float64,\n",
    "        'perc_credit_limit_adjustment' :object,\n",
    "        'color_credit_status'  :object,\n",
    "        'rat_pd'              :object,\n",
    "        'score_pd'            : np.float64,\n",
    "        'has_increased_risk'  :bool,\n",
    "        'is_sole_proprietor'   :bool,\n",
    "        'code_sbi_2'         : np.float64,\n",
    "        'qty_address_mutations_total'  :np.float64,\n",
    "        'qty_address_mutations_month'   :np.float64,\n",
    "        'has_relocated':bool,\n",
    "        'qty_started_names': np.float64,\n",
    "        'qty_stopped_names': np.float64,\n",
    "        'total_changeof_board_members_' :np.float64\n",
    "    }\n",
    "    return dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_parse_dates_list_original_data():\n",
    "    # Setting up dictionary of column types\n",
    "    parse_dates = ['date_established' , 'date_financial_calamity_started',\n",
    "        'date_financial_calamity_stopped', 'date_month', 'date_relocation_last']\n",
    "    return parse_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_one_year_from_bucket_merged_csv(year, dir_prefix = ''):\n",
    "    \"\"\" Reads a whole year of data from the already merged files \"\"\"\n",
    "    dtype = create_dict_types_original_data()\n",
    "    parse_dates = create_parse_dates_list_original_data()\n",
    "    full_year_df = pd.DataFrame()\n",
    "    print('Starting with year: ', year)\n",
    "    print(dir_prefix)\n",
    "    blob_list = list(bucket.list_blobs(prefix=dir_prefix))    \n",
    "    for blob in blob_list:  \n",
    "        print(\"blob\", blob.name)\n",
    "        if year in blob.name:\n",
    "            print('Processing file: ', blob.name)\n",
    "            with fs.open('graydon-data/' + blob.name) as f:\n",
    "                full_year_df = pd.read_csv(f, sep=',', index_col=0, dtype=dtype, parse_dates=parse_dates \n",
    "                                        )   \n",
    "        print('The number of rows so far is: ', full_year_df.shape[0])\n",
    "    return full_year_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ages of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_age_based_on_date(df, col_list):\n",
    "    df['max_date_month'] = df.groupby(['id_branch', 'id_company']).date_month.transform('max')\n",
    "    df['max_date_month_year'] = df['max_date_month'].apply(lambda x: x.year)\n",
    "    for col in col_list:\n",
    "        if col == 'date_established':\n",
    "            df['temp_date_established_year'] = df.date_established.apply(lambda x: x.year)\n",
    "            df['company_age'] = df['max_date_month_year'] - df.temp_date_established_year \n",
    "            df = df.drop(labels =['temp_date_established_year'], axis= 1)\n",
    "        elif col == 'date_relocation_last':\n",
    "            #print(df.columns)\n",
    "            df['max_date_relocation_last'] = df.groupby(['id_branch', 'id_company']).date_relocation_last.transform('max')\n",
    "            df['temp_max_date_relocation_last_year'] = df.max_date_relocation_last.apply(lambda x: x.year)\n",
    "            df['years_in_current_location'] = df['max_date_month_year'] - df.temp_max_date_relocation_last_year \n",
    "            df = df.drop(labels =['temp_max_date_relocation_last_year', 'max_date_relocation_last' ], axis= 1)\n",
    "        elif col == 'year_consolidated_operating_result':            \n",
    "            mask = (df['year_consolidated_operating_result'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_amt_consolidated_operating_result'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_amt_consolidated_operating_result'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_consolidated_operating_result.astype(float))  \n",
    "        elif col == 'year_consolidated_revenue':\n",
    "            mask = (df['year_consolidated_revenue'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_amt_consolidated_revenue'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_amt_consolidated_revenue'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_consolidated_revenue.astype(float))    \n",
    "        elif col == 'year_operating_result':\n",
    "            mask = (df['year_operating_result'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_amt_operating_result'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_amt_operating_result'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_operating_result.astype(float))    \n",
    "        elif col == 'year_qty_employees':\n",
    "            mask = (df['year_qty_employees'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_qty_employees'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_qty_employees'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_qty_employees.astype(float))  \n",
    "        elif col == 'year_revenue':\n",
    "            mask = (df['year_revenue'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_amt_revenue'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_amt_revenue'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_revenue.astype(float)) \n",
    "    df = df.drop(labels =['max_date_month', 'max_date_month_year'], axis= 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_delta_of_column(df, col_list):\n",
    "    subset_columns = ['date_month', 'id_company', 'id_branch']\n",
    "    subset_columns.extend(col_list)\n",
    "    temp_df = df.reset_index().loc[:, subset_columns].sort_values(['id_company','id_branch', 'date_month'])\n",
    "    temp_df = temp_df.groupby(['id_branch', 'id_company']).agg(['first', 'last'])\n",
    "    for col in col_list:\n",
    "        if col == 'qty_employees':\n",
    "            temp_df['delta_qty_employees'] = temp_df['qty_employees']['last'] - temp_df['qty_employees']['first']    \n",
    "        elif col == 'qty_issued_credit_reports':\n",
    "            temp_df['delta_qty_issued_credit_reports'] = (temp_df['qty_issued_credit_reports']['last'] - \n",
    "                                                          temp_df['qty_issued_credit_reports']['first'] )\n",
    "        elif col == 'score_payment_assessment':\n",
    "            temp_df['delta_score_payment_assessment'] = (temp_df['score_payment_assessment']['last'] - \n",
    "                                                          temp_df['score_payment_assessment']['first'] )\n",
    "        elif col == 'score_pd':\n",
    "            temp_df['delta_score_pd'] = (temp_df['score_pd']['last'] - \n",
    "                                                          temp_df['score_pd']['first'] )\n",
    "        elif col == 'code_legal_form':\n",
    "            temp_df['code_legal_form_has_changed'] = (temp_df['code_legal_form']['last'] !=\n",
    "                                                          temp_df['code_legal_form']['first'] )\n",
    "        elif col == 'code_SBI_2_group':\n",
    "            temp_df['SBI_has_changed'] = (temp_df['code_SBI_2_group']['last'] !=\n",
    "                                                          temp_df['code_SBI_2_group']['first'] )   \n",
    "    temp_df.columns = temp_df.columns.droplevel(1)\n",
    "    temp_df = temp_df.loc[:,~temp_df.columns.duplicated()]\n",
    "    temp_df = temp_df.drop(axis=1, columns=col_list)        \n",
    "    df = df.merge(temp_df, how='left', on=['date_month', 'id_company', 'id_branch']) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If any true then true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_if_any_true(df, col_list):\n",
    "    for col in col_list:\n",
    "        if col == 'is_discontinued': \n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['is_discontinued'] \n",
    "                        .any()              # True if any items are True\n",
    "                        .rename('is_discontinued_any')    # name Series \n",
    "                        .to_frame()         # make a dataframe for merging\n",
    "                        .reset_index())\n",
    "        elif col == 'code_financial_calamity':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['code_financial_calamity'] \n",
    "                        .any()            \n",
    "                        .rename('has_financial_calamity')   \n",
    "                        .to_frame() \n",
    "                        .reset_index())\n",
    "        elif col == 'has_relocated':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['has_relocated'] \n",
    "                        .any()            \n",
    "                        .rename('has_relocated_next_year')   \n",
    "                        .to_frame() \n",
    "                        .reset_index())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_mean_of_column(df, col_list):\n",
    "    for col in col_list:\n",
    "        if col == 'amt_consolidated_operating_result':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['amt_consolidated_operating_result'] \n",
    "                        .agg('mean')             \n",
    "                        .rename('mean_amt_consolidated_operating_result')    \n",
    "                        .to_frame()       \n",
    "                        .reset_index())\n",
    "        if col == 'amt_consolidated_revenue':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['amt_consolidated_revenue'] \n",
    "                        .agg('mean')              \n",
    "                        .rename('mean_amt_consolidated_revenue')    \n",
    "                        .to_frame()      \n",
    "                        .reset_index())\n",
    "        if col == 'amt_operating_result':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['amt_operating_result'] \n",
    "                        .agg('mean')           \n",
    "                        .rename('mean_amt_operating_result')    \n",
    "                        .to_frame()         \n",
    "                        .reset_index())\n",
    "        if col == 'amt_revenue':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['amt_revenue']\n",
    "                        .agg('mean')        \n",
    "                        .rename('mean_amt_revenue')   \n",
    "                        .to_frame()     \n",
    "                        .reset_index())\n",
    "        if col == 'qty_employees':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_employees'] \n",
    "                        .agg('mean')          \n",
    "                        .rename('mean_qty_employees')    \n",
    "                        .to_frame()       \n",
    "                        .reset_index())\n",
    "        if col == 'qty_issued_credit_reports':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_issued_credit_reports'] \n",
    "                        .agg('mean')       \n",
    "                        .rename('mean_qty_issued_credit_reports')    \n",
    "                        .to_frame()        \n",
    "                        .reset_index())\n",
    "        if col == 'score_payment_assessment':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['score_payment_assessment'] \n",
    "                        .agg('mean')       \n",
    "                        .rename('mean_score_payment_assessment')    \n",
    "                        .to_frame()        \n",
    "                        .reset_index())\n",
    "        if col == 'score_pd':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['score_pd'] \n",
    "                        .agg('mean')       \n",
    "                        .rename('mean_score_pd')    \n",
    "                        .to_frame()        \n",
    "                        .reset_index())        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummies into counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def column_dummies_into_counts(df, col_list):\n",
    "    df = df.reset_index()\n",
    "    subset_columns = ['id_branch']\n",
    "    subset_columns.extend(col_list)\n",
    "    df['unique_id'] =  df['id_branch'].astype(str) + '_' + df['id_company'].astype(str)\n",
    "    for col in col_list:\n",
    "        temp_df = df.loc[:, subset_columns]\n",
    "        if col == 'color_credit_status':\n",
    "            temp_df = pd.crosstab(df['unique_id'], df['color_credit_status']).reset_index().rename_axis(None,\n",
    "                                                                                                        axis=1).rename(\n",
    "                columns={\"G\": \"qty_green_flags\", \"O\": \"qty_orange_flags\",\"R\": \"qty_red_flags\"})\n",
    "        elif col == 'rat_pd':\n",
    "            temp_df = pd.crosstab(df['unique_id'], df['rat_pd']).reset_index().rename_axis(None, axis=1)\n",
    "        elif col == 'code_SBI_2_group':\n",
    "            temp_df = pd.crosstab(df['unique_id'], df['code_SBI_2_group']).reset_index().rename_axis(None,\n",
    "                                                                                                        axis=1).rename(\n",
    "                columns={\"1\": \"SBI_group_1\", \"2\": \"SBI_group_2\"})\n",
    "        elif col == 'code_legal_form_group':\n",
    "            temp_df = pd.crosstab(df['unique_id'], df['code_legal_form_group']).reset_index().rename_axis(None,\n",
    "                                                                                                        axis=1).rename(\n",
    "                columns={\"1\": \"code_legal_form_group_1\", \"2\": \"code_legal_form_group_2\"})\n",
    "        df = df.merge(temp_df, how='left', on= ['unique_id']) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_ratio_of_column(df, col_list):\n",
    "    for col in col_list:\n",
    "        subset_columns = ['id_branch']\n",
    "        subset_columns.extend(col)\n",
    "        temp_df = df.loc[:, subset_columns]\n",
    "        if col == 'amt_operating_result':\n",
    "            temp_df = df.groupby(['id_branch', 'id_company'])\n",
    "            temp_df = temp_df.agg({'amt_operating_result': 'sum', 'amt_consolidated_operating_result': 'sum'}).rename(\n",
    "    columns={'amt_operating_result': 'sum_amt_operating_result', \n",
    "             'amt_consolidated_operating_result': 'sum_amt_consolidated_operating_result'})\n",
    "            temp_df['ratio_operating_result_consolidated_operating_result'] = np.divide(\n",
    "                temp_df['sum_amt_operating_result'], temp_df['sum_amt_consolidated_operating_result'])\n",
    "            temp_df = temp_df.reset_index()\n",
    "            temp_df = temp_df.drop(axis=1, columns=['sum_amt_consolidated_operating_result', \n",
    "                                                    'sum_amt_operating_result'])\n",
    "            df = df.merge(temp_df, how='left', on= ['id_branch', 'id_company'])  \n",
    "        elif col == 'amt_revenue':\n",
    "            temp_df = df.groupby(['id_branch', 'id_company'])\n",
    "            temp_df = temp_df.agg({'amt_revenue': 'sum', 'amt_consolidated_revenue': 'sum'}).rename(\n",
    "    columns={'amt_revenue': 'sum_amt_revenue', \n",
    "             'amt_consolidated_revenue': 'sum_amt_consolidated_revenue'})\n",
    "            temp_df['ratio_revenue_consolidated_revenue'] = np.divide(temp_df['sum_amt_revenue'],\n",
    "                                                                     temp_df['sum_amt_consolidated_revenue'])\n",
    "            temp_df = temp_df.reset_index()\n",
    "            temp_df = temp_df.drop(axis=1, columns=['sum_amt_revenue', 'sum_amt_consolidated_revenue'])\n",
    "            df = df.merge(temp_df, how='left',  on= ['id_branch', 'id_company'])  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_sum_of_column(df, col_list):\n",
    "    for col in col_list:\n",
    "        if col == 'qty_address_mutations_month':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_address_mutations_month'] \n",
    "                        .agg('sum')             \n",
    "                        .rename('qty_address_mutations_year')    \n",
    "                        .to_frame()       \n",
    "                        .reset_index())\n",
    "        elif col == 'qty_started_names':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_started_names'] \n",
    "                        .agg('sum')              \n",
    "                        .rename('qty_started_names_year')    \n",
    "                        .to_frame()      \n",
    "                        .reset_index())\n",
    "        elif col == 'qty_stopped_names':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_stopped_names'] \n",
    "                        .agg('sum')           \n",
    "                        .rename('qty_stopped_names_year')    \n",
    "                        .to_frame()         \n",
    "                        .reset_index())\n",
    "        elif col == 'total_changeof_board_members_':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['total_changeof_board_members_']\n",
    "                        .agg('sum')        \n",
    "                        .rename('qty_board_changes_year')   \n",
    "                        .to_frame()     \n",
    "                        .reset_index())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_variance_of_column(df, col_list):\n",
    "    for col in col_list:\n",
    "        if col == 'qty_employees':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_employees'] \n",
    "                        .agg('var')             \n",
    "                        .rename('variance_qty_employees')    \n",
    "                        .to_frame()       \n",
    "                        .reset_index())\n",
    "        elif col == 'qty_issued_credit_reports':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_issued_credit_reports'] \n",
    "                        .agg('var')              \n",
    "                        .rename('variance_qty_issued_credit_reports')    \n",
    "                        .to_frame()      \n",
    "                        .reset_index())\n",
    "        elif col == 'score_payment_assessment':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['score_payment_assessment'] \n",
    "                        .agg('sum')           \n",
    "                        .rename('variance_score_payment_assessment')    \n",
    "                        .to_frame()         \n",
    "                        .reset_index())\n",
    "        elif col == 'score_pd':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['score_pd']\n",
    "                        .agg('sum')        \n",
    "                        .rename('variance_score_pd')   \n",
    "                        .to_frame()     \n",
    "                        .reset_index())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get has_relocated from next year DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_has_relocated_with_nextyear(df, next_year, dir_prefix = ''):\n",
    "    dtype={ \n",
    "            'id_branch'    :np.int64,\n",
    "            'id_company'    :np.int64,\n",
    "            'has_relocated':bool\n",
    "    }\n",
    "    full_next_year_df = pd.DataFrame()\n",
    "    cols = ['id_company', 'id_branch', 'has_relocated']\n",
    "    print('Starting withGra year: ', next_year)\n",
    "    print(dir_prefix)\n",
    "    blob_list = list(bucket.list_blobs(prefix=dir_prefix))    \n",
    "    for blob in blob_list:         \n",
    "        if str(next_year) in blob.name:\n",
    "            print('Processing file: ', blob.name)\n",
    "            with fs.open('graydon-data/' + blob.name) as f:\n",
    "                full_next_year_df = pd.read_csv(f, sep=',',  dtype=dtype, usecols= cols\n",
    "                                     )   \n",
    "        print('The number of rows so far is: ', full_next_year_df.shape[0])\n",
    "    full_next_year_df = calculate_if_any_true(full_next_year_df, col_list = ['has_relocated'])\n",
    "    full_next_year_df = full_next_year_df.drop(axis=1, columns='has_relocated')\n",
    "    full_next_year_df = full_next_year_df.drop_duplicates().reset_index().drop(axis=1, columns='index')\n",
    "    df = df.merge(full_next_year_df, on=['id_branch', 'id_company'], how='left', suffixes='_C')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating SBI code groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sbi_groups(df):\n",
    "    code_SBI_2_group1 = [1,19,35,51,53,59,61,62,63,69,72,73,74,78,79,80,82,85,86,87,88,90,93,94]\n",
    "    df['code_SBI_2_group'] = np.where(df['code_sbi_2'].isin(code_SBI_2_group1), \"1\", \"2\")\n",
    "    df = df.drop(axis=1, labels='code_sbi_2', inplace=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating code legal from groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_code_legal_form_groups(df):\n",
    "    code_legal_form_group = [1,4,6,7,8,9,15,17,18]\n",
    "    df['code_legal_form_group'] = np.where(df['code_legal_form'].isin(code_legal_form_group), \"1\", \"2\")\n",
    "    df = df.drop(axis=1, labels='code_legal_form', inplace=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping old columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_old_columns(df, col_list):\n",
    "    df = df.drop(axis=1, labels=col_list, inplace=False)   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deduplicating rows of original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deduplicate_rows(df):\n",
    "    df = df.groupby(['id_branch', 'id_company']).first()\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(axis=1, columns='index')\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning after aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_after_aggregations(df):\n",
    "    df[['has_financial_calamity', 'is_discontinued_any', 'SBI_has_changed'\n",
    "        ,'code_legal_form_has_changed']] = df[['has_financial_calamity', 'is_discontinued_any',\n",
    "                                                'code_legal_form_has_changed', 'SBI_has_changed']].fillna(value=False)\n",
    "    \n",
    "    columns_to_zero = ['mean_qty_issued_credit_reports', 'qty_green_flags', 'qty_orange_flags', \n",
    "                       'qty_red_flags', 'AAA', 'AA', 'A', 'BBB', 'B' , 'CCC', 'CC', 'C', 'D', \n",
    "                       'NR', 'qty_address_mutations_year', 'qty_started_names_year', \n",
    "                       'qty_stopped_names_year', 'qty_board_changes_year', 'code_legal_form_group_1', \n",
    "                       'code_legal_form_group_2', 'SBI_group_1', 'SBI_group_2']\n",
    "    \n",
    "    df[columns_to_zero] = df[columns_to_zero].fillna(value=0)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving DF locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_locally(df, dir_prefix, year, as_json= False):\n",
    "    \"\"\" Saves df as json or csv locally on server \"\"\"\n",
    "    if as_json:        \n",
    "        file_path = dir_prefix + '/' + year + '_aggregated.json'\n",
    "        df.to_json(file_path)\n",
    "    else:\n",
    "        file_path =  dir_prefix + '/' + year + '_aggregated.csv'\n",
    "        df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating dataframe into one year. Main function that calls them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregating dataframe into one year. Main function that calls them all\n",
    "def aggregate_full_year(year, dir_prefix = '', save_df_locally_flag = False):\n",
    "    \n",
    "    next_year = int(year) + 1\n",
    "    \n",
    "    print('Reading DF for year ', year) \n",
    "    df = read_one_year_from_bucket_merged_csv(year, dir_prefix)\n",
    "    \n",
    "    print('Creating SBI groups ')\n",
    "    df = create_sbi_groups(df)\n",
    "    print('Done creating SBI groups')\n",
    "    \n",
    "    print('Calculating delta of variables ')\n",
    "    df = calculate_delta_of_column(df, col_list=['qty_employees','qty_issued_credit_reports', \n",
    "                                                        'score_payment_assessment',\n",
    "                                                       'code_legal_form', 'code_SBI_2_group'])\n",
    "    print('Done calculating delta of variables ') \n",
    "    \n",
    "    print('Creating code legal form groups ')\n",
    "    df = create_code_legal_form_groups(df)\n",
    "    print('Done creating code legal form groups')\n",
    "    \n",
    "    print('Calculating ages of variables ')\n",
    "    df = calculate_age_based_on_date(df,['date_established', 'year_consolidated_operating_result', \n",
    "                                         'year_consolidated_revenue',\n",
    "                                        'year_operating_result', 'year_qty_employees', 'year_revenue', 'date_relocation_last'])\n",
    "    print('Done calculating ages of variables ')\n",
    "    \n",
    "    print('Calculating ratio of columns')\n",
    "    df = calculate_ratio_of_column(df, col_list=['amt_operating_result',\n",
    "                                                        'amt_consolidated_operating_result',\n",
    "                                                         'amt_revenue',\n",
    "                                                        'amt_consolidated_revenue'])\n",
    "    print('Done calculating ratio of columns')\n",
    "        \n",
    "    print('Making dummies into counts')\n",
    "    df = column_dummies_into_counts(df, col_list=['color_credit_status','rat_pd', 'code_legal_form_group',\n",
    "                                                  'code_SBI_2_group'])\n",
    "    print('Done making dummies into counts')\n",
    "    \n",
    " \n",
    "    print('Calculating if any true ')\n",
    "    df = calculate_if_any_true(df, col_list=['is_discontinued', 'code_financial_calamity'])\n",
    "    print('Done calculating if any true ')\n",
    "    \n",
    "    print('Calculating mean of columns ')\n",
    "    df = calculate_mean_of_column(df, col_list=['amt_consolidated_operating_result', \n",
    "                                                        'amt_consolidated_revenue',\n",
    "                                                       'amt_operating_result','amt_revenue',\n",
    "                                                       'qty_employees', 'qty_issued_credit_reports',\n",
    "                                                      'score_payment_assessment' , \n",
    "                                                       'score_pd'])\n",
    "    print('Done calculating mean of columns ')\n",
    "    \n",
    "    print('Calculating sum of columns')\n",
    "    df = calculate_sum_of_column(df, col_list=['qty_address_mutations_month',\n",
    "                                                        'qty_started_names',\n",
    "                                                         'qty_stopped_names',\n",
    "                                                        'total_changeof_board_members_']) \n",
    "    print('Done calculating sum of columns')\n",
    "    \n",
    "    print('Calculating variance of columns')\n",
    "    df = calculate_variance_of_column(df, col_list=['qty_employees',\n",
    "                                                        'qty_issued_credit_reports',\n",
    "                                                         'score_payment_assessment',\n",
    "                                                        'score_pd']) \n",
    "    print('Done calculating variance of columns')\n",
    "\n",
    "\n",
    " \n",
    "    print('Dropping old columns')\n",
    "    df = drop_old_columns(df, col_list = ['date_established', 'year_consolidated_operating_result', \n",
    "                                          'year_consolidated_revenue', \n",
    "                                          'year_operating_result', 'year_qty_employees', 'year_revenue',\n",
    "                                          'is_discontinued', 'code_financial_calamity', \n",
    "                                          'amt_consolidated_operating_result', 'amt_consolidated_revenue', \n",
    "                                          'amt_operating_result','amt_revenue','qty_employees', \n",
    "                                          'qty_issued_credit_reports', 'score_payment_assessment' ,\n",
    "                                          'score_pd', 'color_credit_status','rat_pd',\n",
    "                                          'qty_address_mutations_month','qty_started_names',\n",
    "                                          'qty_stopped_names', 'total_changeof_board_members_', 'is_sole_proprietor',\n",
    "                                          'code_discontinuation','date_financial_calamity_started', \n",
    "                                          'date_financial_calamity_stopped', 'id_company_creditproxy', \n",
    "                                          'financial_calamity_outcome', 'has_increased_risk' , \n",
    "                                          'perc_credit_limit_adjustment',\n",
    "                                          'date_start', 'from_date_start', 'qty_address_mutations_total',\n",
    "                                          'code_legal_form_group', 'code_SBI_2_group', 'date_relocation_last', \n",
    "                                          'date_relocation_penultimate'])\n",
    "    print('Done dropping old columns')\n",
    "\n",
    "    print(' Deduplicating rows of original dataframe')\n",
    "    df = deduplicate_rows(df)\n",
    "    print(' Done deduplicating rows of original dataframe')\n",
    "   \n",
    "    print('Getting target of next year and adding it as a column') \n",
    "    df = replace_has_relocated_with_nextyear(df= df, next_year= next_year,\n",
    "                                   dir_prefix= '02_cleaned')\n",
    "    print('Done getting target of next year and adding it as a column' )\n",
    "    \n",
    "    print(' Cleaning aggregated data frame ')\n",
    "    df = clean_after_aggregations(df)\n",
    "    print('Done cleaning and aggreating dataframe')\n",
    "    \n",
    "    if save_df_locally_flag:\n",
    "        print('Saving DF local to VM into files_to_bucket folder')\n",
    "        save_df_locally(df= df, dir_prefix= 'files_to_bucket/aggregated', year = year)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DF for year  2015\n",
      "Starting with year:  2015\n",
      "02_clean\n",
      "blob 02_cleaned/2013_merged.csv\n",
      "The number of rows so far is:  0\n",
      "blob 02_cleaned/2014_merged.csv\n",
      "The number of rows so far is:  0\n",
      "blob 02_cleaned/2015_merged.csv\n",
      "Processing file:  02_cleaned/2015_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (32,35,36,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows so far is:  21834713\n",
      "blob 02_cleaned/2016_merged.csv\n",
      "The number of rows so far is:  21834713\n",
      "blob 02_cleaned/2017_merged.csv\n",
      "The number of rows so far is:  21834713\n"
     ]
    }
   ],
   "source": [
    "year = '2015'\n",
    "dir_prefix = '02_clean'\n",
    "\n",
    "next_year = int(year) + 1\n",
    "print('Reading DF for year ', year) \n",
    "df = read_one_year_from_bucket_merged_csv(year, dir_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21834713"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SBI groups \n",
      "Done creating SBI groups\n"
     ]
    }
   ],
   "source": [
    "print('Creating SBI groups ')\n",
    "df = create_sbi_groups(df)\n",
    "print('Done creating SBI groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating delta of variables \n",
      "Done calculating delta of variables \n"
     ]
    }
   ],
   "source": [
    "print('Calculating delta of variables ')\n",
    "df = calculate_delta_of_column(df, col_list=['qty_employees','qty_issued_credit_reports', \n",
    "                                                    'score_payment_assessment',\n",
    "                                                   'code_legal_form', 'code_SBI_2_group'])\n",
    "print('Done calculating delta of variables ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating code legal form groups \n",
      "Done creating code legal form groups\n"
     ]
    }
   ],
   "source": [
    "print('Creating code legal form groups ')\n",
    "df = create_code_legal_form_groups(df)\n",
    "print('Done creating code legal form groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ages of variables \n",
      "Done calculating ages of variables \n"
     ]
    }
   ],
   "source": [
    "print('Calculating ages of variables ')\n",
    "df = calculate_age_based_on_date(df,['date_established', 'year_consolidated_operating_result', \n",
    "                                     'year_consolidated_revenue',\n",
    "                                    'year_operating_result', 'year_qty_employees', 'year_revenue', 'date_relocation_last'])\n",
    "print('Done calculating ages of variables ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ratio of columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:23: RuntimeWarning: overflow encountered in true_divide\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done calculating ratio of columns\n"
     ]
    }
   ],
   "source": [
    "print('Calculating ratio of columns')\n",
    "df = calculate_ratio_of_column(df, col_list=['amt_operating_result',\n",
    "                                                    'amt_consolidated_operating_result',\n",
    "                                                     'amt_revenue',\n",
    "                                                    'amt_consolidated_revenue'])\n",
    "print('Done calculating ratio of columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dummies into counts\n",
      "Done making dummies into counts\n"
     ]
    }
   ],
   "source": [
    "print('Making dummies into counts')\n",
    "df = column_dummies_into_counts(df, col_list=['color_credit_status','rat_pd', 'code_legal_form_group',\n",
    "                                              'code_SBI_2_group'])\n",
    "print('Done making dummies into counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating if any true \n",
      "Done calculating if any true \n"
     ]
    }
   ],
   "source": [
    "print('Calculating if any true ')\n",
    "df = calculate_if_any_true(df, col_list=['is_discontinued', 'code_financial_calamity'])\n",
    "print('Done calculating if any true ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating mean of columns \n",
      "Done calculating mean of columns \n"
     ]
    }
   ],
   "source": [
    "print('Calculating mean of columns ')\n",
    "df = calculate_mean_of_column(df, col_list=['amt_consolidated_operating_result', \n",
    "                                                    'amt_consolidated_revenue',\n",
    "                                                   'amt_operating_result','amt_revenue',\n",
    "                                                   'qty_employees', 'qty_issued_credit_reports',\n",
    "                                                  'score_payment_assessment' , \n",
    "                                                   'score_pd'])\n",
    "print('Done calculating mean of columns ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date_month</th>\n",
       "      <th>id_company</th>\n",
       "      <th>id_branch</th>\n",
       "      <th>date_established</th>\n",
       "      <th>is_discontinued</th>\n",
       "      <th>code_discontinuation</th>\n",
       "      <th>code_financial_calamity</th>\n",
       "      <th>date_financial_calamity_started</th>\n",
       "      <th>date_financial_calamity_stopped</th>\n",
       "      <th>financial_calamity_outcome</th>\n",
       "      <th>qty_employees</th>\n",
       "      <th>year_qty_employees</th>\n",
       "      <th>id_company_creditproxy</th>\n",
       "      <th>score_payment_assessment</th>\n",
       "      <th>amt_revenue</th>\n",
       "      <th>year_revenue</th>\n",
       "      <th>amt_operating_result</th>\n",
       "      <th>year_operating_result</th>\n",
       "      <th>amt_consolidated_revenue</th>\n",
       "      <th>year_consolidated_revenue</th>\n",
       "      <th>amt_consolidated_operating_result</th>\n",
       "      <th>year_consolidated_operating_result</th>\n",
       "      <th>qty_issued_credit_reports</th>\n",
       "      <th>perc_credit_limit_adjustment</th>\n",
       "      <th>color_credit_status</th>\n",
       "      <th>rat_pd</th>\n",
       "      <th>score_pd</th>\n",
       "      <th>has_increased_risk</th>\n",
       "      <th>is_sole_proprietor</th>\n",
       "      <th>code_sbi_1</th>\n",
       "      <th>qty_address_mutations_total</th>\n",
       "      <th>qty_address_mutations_month</th>\n",
       "      <th>date_start</th>\n",
       "      <th>from_date_start</th>\n",
       "      <th>has_relocated</th>\n",
       "      <th>qty_started_names</th>\n",
       "      <th>qty_stopped_names</th>\n",
       "      <th>has_name_change</th>\n",
       "      <th>total_changeof_board_members_</th>\n",
       "      <th>date_relocation_last</th>\n",
       "      <th>date_relocation_penultimate</th>\n",
       "      <th>code_SBI_2_group</th>\n",
       "      <th>delta_qty_employees</th>\n",
       "      <th>delta_qty_issued_credit_reports</th>\n",
       "      <th>delta_score_payment_assessment</th>\n",
       "      <th>code_legal_form_has_changed</th>\n",
       "      <th>SBI_has_changed</th>\n",
       "      <th>code_legal_form_group</th>\n",
       "      <th>company_age</th>\n",
       "      <th>years_since_last_amt_consolidated_operating_result</th>\n",
       "      <th>years_since_last_amt_consolidated_revenue</th>\n",
       "      <th>years_since_last_amt_operating_result</th>\n",
       "      <th>years_since_last_qty_employees</th>\n",
       "      <th>years_since_last_amt_revenue</th>\n",
       "      <th>years_in_current_location</th>\n",
       "      <th>ratio_operating_result_consolidated_operating_result</th>\n",
       "      <th>ratio_revenue_consolidated_revenue</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>qty_green_flags</th>\n",
       "      <th>qty_orange_flags</th>\n",
       "      <th>qty_red_flags</th>\n",
       "      <th>A</th>\n",
       "      <th>AA</th>\n",
       "      <th>AAA</th>\n",
       "      <th>B</th>\n",
       "      <th>BB</th>\n",
       "      <th>BBB</th>\n",
       "      <th>C</th>\n",
       "      <th>CC</th>\n",
       "      <th>CCC</th>\n",
       "      <th>D</th>\n",
       "      <th>NR</th>\n",
       "      <th>code_legal_form_group_1</th>\n",
       "      <th>code_legal_form_group_2</th>\n",
       "      <th>SBI_group_1</th>\n",
       "      <th>SBI_group_2</th>\n",
       "      <th>is_discontinued_any</th>\n",
       "      <th>has_financial_calamity</th>\n",
       "      <th>mean_amt_consolidated_operating_result</th>\n",
       "      <th>mean_amt_consolidated_revenue</th>\n",
       "      <th>mean_amt_operating_result</th>\n",
       "      <th>mean_amt_revenue</th>\n",
       "      <th>mean_qty_employees</th>\n",
       "      <th>mean_qty_issued_credit_reports</th>\n",
       "      <th>mean_score_payment_assessment</th>\n",
       "      <th>mean_score_pd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10079408</td>\n",
       "      <td>1921-03-17</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.552530e-316</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>9.219265e-318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>G</td>\n",
       "      <td>BBB</td>\n",
       "      <td>-4.907</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-06-11</td>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>2</td>\n",
       "      <td>993.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>94.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>10079408_3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.219265e-318</td>\n",
       "      <td>3.552530e-316</td>\n",
       "      <td>834.5</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-4.956917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1795335</td>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10079408</td>\n",
       "      <td>1921-03-17</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.552530e-316</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>9.219265e-318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>G</td>\n",
       "      <td>BBB</td>\n",
       "      <td>-4.940</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-06-11</td>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>94.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>10079408_3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.219265e-318</td>\n",
       "      <td>3.552530e-316</td>\n",
       "      <td>834.5</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-4.956917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3597202</td>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10079408</td>\n",
       "      <td>1921-03-17</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.552530e-316</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>9.219265e-318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30</td>\n",
       "      <td>G</td>\n",
       "      <td>BBB</td>\n",
       "      <td>-4.963</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-06-11</td>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>94.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>10079408_3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.219265e-318</td>\n",
       "      <td>3.552530e-316</td>\n",
       "      <td>834.5</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-4.956917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5403385</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10079408</td>\n",
       "      <td>1921-03-17</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.552530e-316</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>9.219265e-318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30</td>\n",
       "      <td>G</td>\n",
       "      <td>BBB</td>\n",
       "      <td>-4.969</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-06-11</td>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>94.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>10079408_3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.219265e-318</td>\n",
       "      <td>3.552530e-316</td>\n",
       "      <td>834.5</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-4.956917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7215415</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10079408</td>\n",
       "      <td>1921-03-17</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.552530e-316</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>9.219265e-318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30</td>\n",
       "      <td>G</td>\n",
       "      <td>BBB</td>\n",
       "      <td>-4.969</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-06-11</td>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>94.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>10079408_3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.219265e-318</td>\n",
       "      <td>3.552530e-316</td>\n",
       "      <td>834.5</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-4.956917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HTML(DataFrame(df).head(5).to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.71 s ± 413 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "#df_count = df.groupby(['id_company', 'id_branch', 'color_credit_status']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_count.columns = ['qty_flags_green', 'qty_flags_orange', 'qty_flags_red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1890941"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_count.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21834713"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_test = df.merge(df_count, on=['id_company', 'id_branch'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating sum of columns\n",
      "Done calculating sum of columns\n"
     ]
    }
   ],
   "source": [
    "print('Calculating sum of columns')\n",
    "df = calculate_sum_of_column(df, col_list=['qty_address_mutations_month',\n",
    "                                                    'qty_started_names',\n",
    "                                                     'qty_stopped_names',\n",
    "                                                    'total_changeof_board_members_']) \n",
    "print('Done calculating sum of columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating variance of columns\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-e1d2a9006b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                     \u001b[0;34m'qty_issued_credit_reports'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                      \u001b[0;34m'score_payment_assessment'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                     'score_pd']) \n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calculating variance of columns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ec3e9d67fd27>\u001b[0m in \u001b[0;36mcalculate_variance_of_column\u001b[0;34m(df, col_list)\u001b[0m\n\u001b[1;32m     16\u001b[0m             df = df.merge(df.groupby(['id_branch', 'id_company'])['score_payment_assessment'] \n\u001b[1;32m     17\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                         \u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variance_score_payment_assessment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                         .reset_index())\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   6387\u001b[0m                      \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6388\u001b[0m                      \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6389\u001b[0;31m                      copy=copy, indicator=indicator, validate=validate)\n\u001b[0m\u001b[1;32m   6390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     60\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                          validate=validate)\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mllabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             concat_axis=0, copy=self.copy)\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5420\u001b[0m             b = make_block(\n\u001b[0;32m-> 5421\u001b[0;31m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5422\u001b[0m                 placement=placement)\n\u001b[1;32m   5423\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5563\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[1;32m   5564\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[0;32m-> 5565\u001b[0;31m                  for ju in join_units]\n\u001b[0m\u001b[1;32m   5566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5567\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   5563\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[1;32m   5564\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[0;32m-> 5565\u001b[0;31m                  for ju in join_units]\n\u001b[0m\u001b[1;32m   5566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5567\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m   5873\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5874\u001b[0m                 values = algos.take_nd(values, indexer, axis=ax,\n\u001b[0;32m-> 5875\u001b[0;31m                                        fill_value=fill_value)\n\u001b[0m\u001b[1;32m   5876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5877\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\n\u001b[1;32m   1658\u001b[0m                                  mask_info=mask_info)\n\u001b[0;32m-> 1659\u001b[0;31m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflip_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Calculating variance of columns')\n",
    "df = calculate_variance_of_column(df, col_list=['qty_employees',\n",
    "                                                    'qty_issued_credit_reports',\n",
    "                                                     'score_payment_assessment',\n",
    "                                                    'score_pd']) \n",
    "print('Done calculating variance of columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Dropping old columns')\n",
    "df = drop_old_columns(df, col_list = ['date_established', 'year_consolidated_operating_result', \n",
    "                                      'year_consolidated_revenue', \n",
    "                                      'year_operating_result', 'year_qty_employees', 'year_revenue',\n",
    "                                      'is_discontinued', 'code_financial_calamity', \n",
    "                                      'amt_consolidated_operating_result', 'amt_consolidated_revenue', \n",
    "                                      'amt_operating_result','amt_revenue','qty_employees', \n",
    "                                      'qty_issued_credit_reports', 'score_payment_assessment' ,\n",
    "                                      'score_pd', 'color_credit_status','rat_pd',\n",
    "                                      'qty_address_mutations_month','qty_started_names',\n",
    "                                      'qty_stopped_names', 'total_changeof_board_members_', 'is_sole_proprietor',\n",
    "                                      'code_discontinuation','date_financial_calamity_started', \n",
    "                                      'date_financial_calamity_stopped', 'id_company_creditproxy', \n",
    "                                      'financial_calamity_outcome', 'has_increased_risk' , \n",
    "                                      'perc_credit_limit_adjustment',\n",
    "                                      'date_start', 'from_date_start', 'qty_address_mutations_total',\n",
    "                                      'code_legal_form_group', 'code_SBI_2_group', 'date_relocation_last', \n",
    "                                      'date_relocation_penultimate'])\n",
    "print('Done dropping old columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(' Deduplicating rows of original dataframe')\n",
    "df = deduplicate_rows(df)\n",
    "print(' Done deduplicating rows of original dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Getting target of next year and adding it as a column') \n",
    "df = replace_has_relocated_with_nextyear(df= df, next_year= next_year,\n",
    "                               dir_prefix= '02_cleaned')\n",
    "print('Done getting target of next year and adding it as a column' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(' Cleaning aggregated data frame ')\n",
    "df = clean_after_aggregations(df)\n",
    "print('Done cleaning and aggreating dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Saving DF local to VM into files_to_bucket folder')\n",
    "save_df_locally(df= df, dir_prefix= 'files_to_bucket/aggregated', year = year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DF for year  2015\n",
      "Starting with year:  2015\n",
      "02_clean\n",
      "blob 02_cleaned/2013_merged.csv\n",
      "The number of rows so far is:  0\n",
      "blob 02_cleaned/2014_merged.csv\n",
      "The number of rows so far is:  0\n",
      "blob 02_cleaned/2015_merged.csv\n",
      "Processing file:  02_cleaned/2015_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/IPython/core/magics/execution.py:1246: DtypeWarning: Columns (32,35,36,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code, glob, local_ns)\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows so far is:  21834713\n",
      "blob 02_cleaned/2016_merged.csv\n",
      "The number of rows so far is:  21834713\n",
      "blob 02_cleaned/2017_merged.csv\n",
      "The number of rows so far is:  21834713\n",
      "Creating SBI groups \n",
      "Done creating SBI groups\n",
      "Calculating delta of variables \n",
      "Done calculating delta of variables \n",
      "Creating code legal form groups \n",
      "Done creating code legal form groups\n",
      "Calculating ages of variables \n",
      "Done calculating ages of variables \n",
      "Calculating ratio of columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:23: RuntimeWarning: overflow encountered in true_divide\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done calculating ratio of columns\n",
      "Making dummies into counts\n",
      "Done making dummies into counts\n",
      "Calculating if any true \n",
      "Done calculating if any true \n",
      "Calculating mean of columns \n",
      "Done calculating mean of columns \n",
      "Calculating sum of columns\n",
      "Done calculating sum of columns\n",
      "Calculating variance of columns\n",
      "Done calculating variance of columns\n",
      "Dropping old columns\n",
      "Done dropping old columns\n",
      " Deduplicating rows of original dataframe\n",
      " Done deduplicating rows of original dataframe\n",
      "Getting target of next year and adding it as a column\n",
      "Starting withGra year:  2016\n",
      "including_scores/merged_per_year/merged_cleaned/relocation_dates\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id_branch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-68b71c7cad1b>\u001b[0m in \u001b[0;36maggregate_full_year\u001b[0;34m(year, dir_prefix, save_df_locally_flag)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Getting target of next year and adding it as a column'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     df = replace_has_relocated_with_nextyear(df= df, next_year= next_year,\n\u001b[0;32m---> 97\u001b[0;31m                                    dir_prefix= 'including_scores/merged_per_year/merged_cleaned/relocation_dates')\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done getting target of next year and adding it as a column'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-e6843064e4f6>\u001b[0m in \u001b[0;36mreplace_has_relocated_with_nextyear\u001b[0;34m(df, next_year, dir_prefix)\u001b[0m\n\u001b[1;32m     17\u001b[0m                                      )   \n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The number of rows so far is: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_next_year_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mfull_next_year_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_if_any_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_next_year_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'has_relocated'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mfull_next_year_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_next_year_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'has_relocated'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mfull_next_year_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_next_year_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-eeac81fa0e5e>\u001b[0m in \u001b[0;36mcalculate_if_any_true\u001b[0;34m(df, col_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m                         .reset_index())\n\u001b[1;32m     15\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'has_relocated'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             df = df.merge(df.groupby(['id_branch', 'id_company'])['has_relocated'] \n\u001b[0m\u001b[1;32m     17\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'has_relocated_next_year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m   6663\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n\u001b[1;32m   6664\u001b[0m                        \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6665\u001b[0;31m                        observed=observed, **kwargs)\n\u001b[0m\u001b[1;32m   6666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6667\u001b[0m     def asfreq(self, freq, method=None, how=None, normalize=False,\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(obj, by, **kwds)\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid type: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                                                     \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                     \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                                                     mutated=self.mutated)\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate)\u001b[0m\n\u001b[1;32m   3289\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3290\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3291\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3292\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3293\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id_branch'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "one_year_df = aggregate_full_year(year = '2015', dir_prefix= '02_clean',\n",
    "                                  save_df_locally_flag= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previewing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(DataFrame(one_year_df).head(100).to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(one_year_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### one_year_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graydon-moving",
   "language": "python",
   "name": "graydon-moving"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
