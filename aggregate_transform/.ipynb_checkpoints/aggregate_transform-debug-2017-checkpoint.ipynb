{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ignore 'dask' warning\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "from google.cloud import storage\n",
    "from pandas import DataFrame\n",
    "from IPython.display import HTML\n",
    "from google.cloud.storage import Blob\n",
    "import datalab.storage as gcs_datalab\n",
    "from datetime import date\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up constants. All required\n",
    "project = 'graydon-moving-indicator'\n",
    "bucket_name = 'graydon-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing bucket\n",
    "fs = gcsfs.GCSFileSystem(project='graydon-moving-indicator')\n",
    "gcs = storage.Client()\n",
    "bucket = gcs.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-aggregation\n",
    "def create_dict_types_original_data():\n",
    "    # Setting up dictionary of column types\n",
    "    dtype={ 'id_company'  :np.float64,\n",
    "        'id_branch'    :np.int64,\n",
    "        'is_discontinued':bool,\n",
    "        'code_discontinuation': np.float64,\n",
    "        'code_financial_calamity':object,\n",
    "        'financial_calamity_outcome'   : np.float64,\n",
    "        'code_legal_form' : np.float64,\n",
    "        'qty_employees' :np.float64,\n",
    "        'year_qty_employees' :np.float64,\n",
    "        'id_company_creditproxy':object,\n",
    "        'score_payment_assessment'    : np.float64,\n",
    "        'amt_revenue'  : np.float64,\n",
    "        'year_revenue'  : np.float64,\n",
    "        'amt_operating_result'   : np.float64,\n",
    "        'year_operating_result'    :object,\n",
    "        'amt_consolidated_revenue'   : np.float64,\n",
    "        'year_consolidated_revenue'   :object,\n",
    "        'amt_consolidated_operating_result'     : np.float64,\n",
    "        'year_consolidated_operating_result'   :object,\n",
    "        'qty_issued_credit_reports' : np.float64,\n",
    "        'perc_credit_limit_adjustment' :object,\n",
    "        'color_credit_status'  :object,\n",
    "        'rat_pd'              :object,\n",
    "        'score_pd'            : np.float64,\n",
    "        'has_increased_risk'  :bool,\n",
    "        'is_sole_proprietor'   :bool,\n",
    "        'code_sbi_2'         : np.float64,\n",
    "        'qty_address_mutations_total'  :np.float64,\n",
    "        'qty_address_mutations_month'   :np.float64,\n",
    "        'has_relocated':bool,\n",
    "        'qty_started_names': np.float64,\n",
    "        'qty_stopped_names': np.float64,\n",
    "        'total_changeof_board_members_' :np.float64\n",
    "    }\n",
    "    return dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_parse_dates_list_original_data():\n",
    "    # Setting up dictionary of column types\n",
    "    parse_dates = ['date_established' , 'date_financial_calamity_started',\n",
    "        'date_financial_calamity_stopped', 'date_month', 'date_relocation_last']\n",
    "    return parse_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_one_year_from_bucket_merged_csv(year, dir_prefix = ''):\n",
    "    \"\"\" Reads a whole year of data from the already merged files \"\"\"\n",
    "    dtype = create_dict_types_original_data()\n",
    "    parse_dates = create_parse_dates_list_original_data()\n",
    "    full_year_df = pd.DataFrame()\n",
    "    print('Starting with year: ', year)\n",
    "    print(dir_prefix)\n",
    "    blob_list = list(bucket.list_blobs(prefix=dir_prefix))    \n",
    "    for blob in blob_list:  \n",
    "        print(\"blob\", blob.name)\n",
    "        if year in blob.name:\n",
    "            print('Processing file: ', blob.name)\n",
    "            with fs.open('graydon-data/' + blob.name) as f:\n",
    "                full_year_df = pd.read_csv(f, sep=',', index_col=0, dtype=dtype, parse_dates=parse_dates \n",
    "                                        )   \n",
    "        print('The number of rows so far is: ', full_year_df.shape[0])\n",
    "    return full_year_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ages of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_age_based_on_date(df, col_list):\n",
    "    df['max_date_month'] = df.groupby(['id_branch', 'id_company']).date_month.transform('max')\n",
    "    df['max_date_month_year'] = df['max_date_month'].apply(lambda x: x.year)\n",
    "    for col in col_list:\n",
    "        if col == 'date_established':\n",
    "            df['temp_date_established_year'] = df.date_established.apply(lambda x: x.year)\n",
    "            df['company_age'] = df['max_date_month_year'] - df.temp_date_established_year \n",
    "            df = df.drop(labels =['temp_date_established_year'], axis= 1)\n",
    "        elif col == 'date_relocation_last':\n",
    "            #print(df.columns)\n",
    "            df['max_date_relocation_last'] = df.groupby(['id_branch', 'id_company']).date_relocation_last.transform('max')\n",
    "            df['temp_max_date_relocation_last_year'] = df.max_date_relocation_last.apply(lambda x: x.year)\n",
    "            df['years_in_current_location'] = df['max_date_month_year'] - df.temp_max_date_relocation_last_year \n",
    "            df = df.drop(labels =['temp_max_date_relocation_last_year', 'max_date_relocation_last' ], axis= 1)\n",
    "        elif col == 'year_consolidated_operating_result':            \n",
    "            mask = (df['year_consolidated_operating_result'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_amt_consolidated_operating_result'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_amt_consolidated_operating_result'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_consolidated_operating_result.astype(float))  \n",
    "        elif col == 'year_consolidated_revenue':\n",
    "            mask = (df['year_consolidated_revenue'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_amt_consolidated_revenue'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_amt_consolidated_revenue'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_consolidated_revenue.astype(float))    \n",
    "        elif col == 'year_operating_result':\n",
    "            mask = (df['year_operating_result'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_amt_operating_result'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_amt_operating_result'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_operating_result.astype(float))    \n",
    "        elif col == 'year_qty_employees':\n",
    "            mask = (df['year_qty_employees'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_qty_employees'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_qty_employees'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_qty_employees.astype(float))  \n",
    "        elif col == 'year_revenue':\n",
    "            mask = (df['year_revenue'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_amt_revenue'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_amt_revenue'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_revenue.astype(float)) \n",
    "    df = df.drop(labels =['max_date_month', 'max_date_month_year'], axis= 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_delta_of_column(df, col_list):\n",
    "    subset_columns = ['date_month', 'id_company', 'id_branch']\n",
    "    subset_columns.extend(col_list)\n",
    "    temp_df = df.reset_index().loc[:, subset_columns].sort_values(['id_company','id_branch', 'date_month'])\n",
    "    temp_df = temp_df.groupby(['id_branch', 'id_company']).agg(['first', 'last'])\n",
    "    for col in col_list:\n",
    "        if col == 'qty_employees':\n",
    "            temp_df['delta_qty_employees'] = temp_df['qty_employees']['last'] - temp_df['qty_employees']['first']    \n",
    "        elif col == 'qty_issued_credit_reports':\n",
    "            temp_df['delta_qty_issued_credit_reports'] = (temp_df['qty_issued_credit_reports']['last'] - \n",
    "                                                          temp_df['qty_issued_credit_reports']['first'] )\n",
    "        elif col == 'score_payment_assessment':\n",
    "            temp_df['delta_score_payment_assessment'] = (temp_df['score_payment_assessment']['last'] - \n",
    "                                                          temp_df['score_payment_assessment']['first'] )\n",
    "        elif col == 'score_pd':\n",
    "            temp_df['delta_score_pd'] = (temp_df['score_pd']['last'] - \n",
    "                                                          temp_df['score_pd']['first'] )\n",
    "        elif col == 'code_legal_form':\n",
    "            temp_df['code_legal_form_has_changed'] = (temp_df['code_legal_form']['last'] !=\n",
    "                                                          temp_df['code_legal_form']['first'] )\n",
    "        elif col == 'code_SBI_2_group':\n",
    "            temp_df['SBI_has_changed'] = (temp_df['code_SBI_2_group']['last'] !=\n",
    "                                                          temp_df['code_SBI_2_group']['first'] )   \n",
    "    temp_df.columns = temp_df.columns.droplevel(1)\n",
    "    temp_df = temp_df.loc[:,~temp_df.columns.duplicated()]\n",
    "    temp_df = temp_df.drop(axis=1, columns=col_list)        \n",
    "    df = df.merge(temp_df, how='left', on=['date_month', 'id_company', 'id_branch']) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If any true then true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_if_any_true(df, col_list):\n",
    "    for col in col_list:\n",
    "        if col == 'is_discontinued': \n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['is_discontinued'] \n",
    "                        .any()              # True if any items are True\n",
    "                        .rename('is_discontinued_any')    # name Series \n",
    "                        .to_frame()         # make a dataframe for merging\n",
    "                        .reset_index())\n",
    "        elif col == 'code_financial_calamity':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['code_financial_calamity'] \n",
    "                        .any()            \n",
    "                        .rename('has_financial_calamity')   \n",
    "                        .to_frame() \n",
    "                        .reset_index())\n",
    "        elif col == 'has_relocated':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['has_relocated'] \n",
    "                        .any()            \n",
    "                        .rename('has_relocated_next_year')   \n",
    "                        .to_frame() \n",
    "                        .reset_index())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_mean_of_column(df, col_list):\n",
    "    for col in col_list:\n",
    "        if col == 'amt_consolidated_operating_result':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['amt_consolidated_operating_result'] \n",
    "                        .agg('mean')             \n",
    "                        .rename('mean_amt_consolidated_operating_result')    \n",
    "                        .to_frame()       \n",
    "                        .reset_index())\n",
    "        if col == 'amt_consolidated_revenue':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['amt_consolidated_revenue'] \n",
    "                        .agg('mean')              \n",
    "                        .rename('mean_amt_consolidated_revenue')    \n",
    "                        .to_frame()      \n",
    "                        .reset_index())\n",
    "        if col == 'amt_operating_result':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['amt_operating_result'] \n",
    "                        .agg('mean')           \n",
    "                        .rename('mean_amt_operating_result')    \n",
    "                        .to_frame()         \n",
    "                        .reset_index())\n",
    "        if col == 'amt_revenue':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['amt_revenue']\n",
    "                        .agg('mean')        \n",
    "                        .rename('mean_amt_revenue')   \n",
    "                        .to_frame()     \n",
    "                        .reset_index())\n",
    "        if col == 'qty_employees':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_employees'] \n",
    "                        .agg('mean')          \n",
    "                        .rename('mean_qty_employees')    \n",
    "                        .to_frame()       \n",
    "                        .reset_index())\n",
    "        if col == 'qty_issued_credit_reports':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_issued_credit_reports'] \n",
    "                        .agg('mean')       \n",
    "                        .rename('mean_qty_issued_credit_reports')    \n",
    "                        .to_frame()        \n",
    "                        .reset_index())\n",
    "        if col == 'score_payment_assessment':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['score_payment_assessment'] \n",
    "                        .agg('mean')       \n",
    "                        .rename('mean_score_payment_assessment')    \n",
    "                        .to_frame()        \n",
    "                        .reset_index())\n",
    "        if col == 'score_pd':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['score_pd'] \n",
    "                        .agg('mean')       \n",
    "                        .rename('mean_score_pd')    \n",
    "                        .to_frame()        \n",
    "                        .reset_index())        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummies into counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def column_dummies_into_counts(df, col_list):\n",
    "    df = df.reset_index()\n",
    "    subset_columns = ['id_branch']\n",
    "    subset_columns.extend(col_list)\n",
    "    df['unique_id'] =  df['id_branch'].astype(str) + '_' + df['id_company'].astype(str)\n",
    "    for col in col_list:\n",
    "        temp_df = df.loc[:, subset_columns]\n",
    "        if col == 'color_credit_status':\n",
    "            temp_df = pd.crosstab(df['unique_id'], df['color_credit_status']).reset_index().rename_axis(None,\n",
    "                                                                                                        axis=1).rename(\n",
    "                columns={\"G\": \"qty_green_flags\", \"O\": \"qty_orange_flags\",\"R\": \"qty_red_flags\"})\n",
    "        elif col == 'rat_pd':\n",
    "            temp_df = pd.crosstab(df['unique_id'], df['rat_pd']).reset_index().rename_axis(None, axis=1)\n",
    "        elif col == 'code_SBI_2_group':\n",
    "            temp_df = pd.crosstab(df['unique_id'], df['code_SBI_2_group']).reset_index().rename_axis(None,\n",
    "                                                                                                        axis=1).rename(\n",
    "                columns={\"1\": \"SBI_group_1\", \"2\": \"SBI_group_2\"})\n",
    "        elif col == 'code_legal_form_group':\n",
    "            temp_df = pd.crosstab(df['unique_id'], df['code_legal_form_group']).reset_index().rename_axis(None,\n",
    "                                                                                                        axis=1).rename(\n",
    "                columns={\"1\": \"code_legal_form_group_1\", \"2\": \"code_legal_form_group_2\"})\n",
    "        df = df.merge(temp_df, how='left', on= ['unique_id']) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_ratio_of_column(df, col_list):\n",
    "    for col in col_list:\n",
    "        subset_columns = ['id_branch']\n",
    "        subset_columns.extend(col)\n",
    "        temp_df = df.loc[:, subset_columns]\n",
    "        if col == 'amt_operating_result':\n",
    "            temp_df = df.groupby(['id_branch', 'id_company'])\n",
    "            temp_df = temp_df.agg({'amt_operating_result': 'sum', 'amt_consolidated_operating_result': 'sum'}).rename(\n",
    "    columns={'amt_operating_result': 'sum_amt_operating_result', \n",
    "             'amt_consolidated_operating_result': 'sum_amt_consolidated_operating_result'})\n",
    "            temp_df['ratio_operating_result_consolidated_operating_result'] = np.divide(\n",
    "                temp_df['sum_amt_operating_result'], temp_df['sum_amt_consolidated_operating_result'])\n",
    "            temp_df = temp_df.reset_index()\n",
    "            temp_df = temp_df.drop(axis=1, columns=['sum_amt_consolidated_operating_result', \n",
    "                                                    'sum_amt_operating_result'])\n",
    "            df = df.merge(temp_df, how='left', on= ['id_branch', 'id_company'])  \n",
    "        elif col == 'amt_revenue':\n",
    "            temp_df = df.groupby(['id_branch', 'id_company'])\n",
    "            temp_df = temp_df.agg({'amt_revenue': 'sum', 'amt_consolidated_revenue': 'sum'}).rename(\n",
    "    columns={'amt_revenue': 'sum_amt_revenue', \n",
    "             'amt_consolidated_revenue': 'sum_amt_consolidated_revenue'})\n",
    "            temp_df['ratio_revenue_consolidated_revenue'] = np.divide(temp_df['sum_amt_revenue'],\n",
    "                                                                     temp_df['sum_amt_consolidated_revenue'])\n",
    "            temp_df = temp_df.reset_index()\n",
    "            temp_df = temp_df.drop(axis=1, columns=['sum_amt_revenue', 'sum_amt_consolidated_revenue'])\n",
    "            df = df.merge(temp_df, how='left',  on= ['id_branch', 'id_company'])  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_sum_of_column(df, col_list):\n",
    "    # List of 'based on'-'write to' column pairs to read-write values from-to\n",
    "    list_column_pairs = [['qty_address_mutations_month', 'qty_address_mutations_year'],\n",
    "                         ['qty_started_names', 'qty_started_names_year'],\n",
    "                         ['qty_stopped_names', 'qty_stopped_names_year'],\n",
    "                         ['total_changeof_board_members_', 'qty_board_changes_year']]\n",
    "    key_aggregattion = ['id_branch', 'id_company']\n",
    "    for column_pair in list_column_pairs:\n",
    "        df = df.merge(df.groupby(key_aggregattion)[column_pair[0]] \n",
    "                      .agg('sum')             \n",
    "                      .rename(column_pair[1])    \n",
    "                      .to_frame()       \n",
    "                      .reset_index())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_variance_of_column(df, col_list):\n",
    "    for col in col_list:\n",
    "        if col == 'qty_employees':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_employees'] \n",
    "                        .agg('var')             \n",
    "                        .rename('variance_qty_employees')    \n",
    "                        .to_frame()       \n",
    "                        .reset_index())\n",
    "        elif col == 'qty_issued_credit_reports':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_issued_credit_reports'] \n",
    "                        .agg('var')              \n",
    "                        .rename('variance_qty_issued_credit_reports')    \n",
    "                        .to_frame()      \n",
    "                        .reset_index())\n",
    "        elif col == 'score_payment_assessment':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['score_payment_assessment'] \n",
    "                        .agg('sum')           \n",
    "                        .rename('variance_score_payment_assessment')    \n",
    "                        .to_frame()         \n",
    "                        .reset_index())\n",
    "        elif col == 'score_pd':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['score_pd']\n",
    "                        .agg('sum')        \n",
    "                        .rename('variance_score_pd')   \n",
    "                        .to_frame()     \n",
    "                        .reset_index())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get has_relocated from next year DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_has_relocated_with_nextyear(df, next_year, dir_prefix = ''):\n",
    "    dtype={ \n",
    "            'id_branch'    :np.int64,\n",
    "            'id_company'    :np.int64,\n",
    "            'has_relocated':bool\n",
    "    }\n",
    "    full_next_year_df = pd.DataFrame()\n",
    "    cols = ['id_company', 'id_branch', 'has_relocated']\n",
    "    print('Starting withGra year: ', next_year)\n",
    "    print(dir_prefix)\n",
    "    blob_list = list(bucket.list_blobs(prefix=dir_prefix))    \n",
    "    for blob in blob_list:         \n",
    "        if str(next_year) in blob.name:\n",
    "            print('Processing file: ', blob.name)\n",
    "            with fs.open('graydon-data/' + blob.name) as f:\n",
    "                full_next_year_df = pd.read_csv(f, sep=',',  dtype=dtype, usecols= cols\n",
    "                                     )   \n",
    "        print('The number of rows so far is: ', full_next_year_df.shape[0])\n",
    "    full_next_year_df = calculate_if_any_true(full_next_year_df, col_list = ['has_relocated'])\n",
    "    full_next_year_df = full_next_year_df.drop(axis=1, columns='has_relocated')\n",
    "    full_next_year_df = full_next_year_df.drop_duplicates().reset_index().drop(axis=1, columns='index')\n",
    "    df = df.merge(full_next_year_df, on=['id_branch', 'id_company'], how='left', suffixes='_C')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating SBI code groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sbi_groups(df):\n",
    "    code_SBI_2_group1 = [1,19,35,51,53,59,61,62,63,69,72,73,74,78,79,80,82,85,86,87,88,90,93,94]\n",
    "    df['code_SBI_2_group'] = np.where(df['code_sbi_2'].isin(code_SBI_2_group1), \"1\", \"2\")\n",
    "    df = df.drop(axis=1, labels='code_sbi_2', inplace=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating code legal from groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_code_legal_form_groups(df):\n",
    "    code_legal_form_group = [1,4,6,7,8,9,15,17,18]\n",
    "    df['code_legal_form_group'] = np.where(df['code_legal_form'].isin(code_legal_form_group), \"1\", \"2\")\n",
    "    df = df.drop(axis=1, labels='code_legal_form', inplace=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping old columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_old_columns(df, col_list):\n",
    "    df = df.drop(axis=1, labels=col_list, inplace=False)   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deduplicating rows of original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deduplicate_rows(df):\n",
    "    df = df.groupby(['id_branch', 'id_company']).first()\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(axis=1, columns='index')\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning after aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_after_aggregations(df):\n",
    "    df[['has_financial_calamity', 'is_discontinued_any', 'SBI_has_changed'\n",
    "        ,'code_legal_form_has_changed']] = df[['has_financial_calamity', 'is_discontinued_any',\n",
    "                                                'code_legal_form_has_changed', 'SBI_has_changed']].fillna(value=False)\n",
    "    \n",
    "    columns_to_zero = ['mean_qty_issued_credit_reports', 'qty_green_flags', 'qty_orange_flags', \n",
    "                       'qty_red_flags', 'AAA', 'AA', 'A', 'BBB', 'B' , 'CCC', 'CC', 'C', 'D', \n",
    "                       'NR', 'qty_address_mutations_year', 'qty_started_names_year', \n",
    "                       'qty_stopped_names_year', 'qty_board_changes_year', 'code_legal_form_group_1', \n",
    "                       'code_legal_form_group_2', 'SBI_group_1', 'SBI_group_2']\n",
    "    \n",
    "    df[columns_to_zero] = df[columns_to_zero].fillna(value=0)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving DF locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_locally(df, dir_prefix, year, as_json= False):\n",
    "    \"\"\" Saves df as json or csv locally on server \"\"\"\n",
    "    if as_json:        \n",
    "        file_path = dir_prefix + '/' + year + '_aggregated.json'\n",
    "        df.to_json(file_path)\n",
    "    else:\n",
    "        file_path =  dir_prefix + '/' + year + '_aggregated.csv'\n",
    "        df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating dataframe into one year. Main function that calls them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregating dataframe into one year. Main function that calls them all\n",
    "def aggregate_full_year(year, dir_prefix = '', save_df_locally_flag = False):\n",
    "    \n",
    "    next_year = int(year) + 1\n",
    "    \n",
    "    print('Reading DF for year ', year) \n",
    "    df = read_one_year_from_bucket_merged_csv(year, dir_prefix)\n",
    "    \n",
    "    print('Creating SBI groups ')\n",
    "    df = create_sbi_groups(df)\n",
    "    print('Done creating SBI groups')\n",
    "    \n",
    "    print('Calculating delta of variables ')\n",
    "    df = calculate_delta_of_column(df, col_list=['qty_employees','qty_issued_credit_reports', \n",
    "                                                        'score_payment_assessment',\n",
    "                                                       'code_legal_form', 'code_SBI_2_group'])\n",
    "    print('Done calculating delta of variables ') \n",
    "    \n",
    "    print('Creating code legal form groups ')\n",
    "    df = create_code_legal_form_groups(df)\n",
    "    print('Done creating code legal form groups')\n",
    "    \n",
    "    print('Calculating ages of variables ')\n",
    "    df = calculate_age_based_on_date(df,['date_established', 'year_consolidated_operating_result', \n",
    "                                         'year_consolidated_revenue',\n",
    "                                        'year_operating_result', 'year_qty_employees', 'year_revenue', 'date_relocation_last'])\n",
    "    print('Done calculating ages of variables ')\n",
    "    \n",
    "    print('Calculating ratio of columns')\n",
    "    df = calculate_ratio_of_column(df, col_list=['amt_operating_result',\n",
    "                                                        'amt_consolidated_operating_result',\n",
    "                                                         'amt_revenue',\n",
    "                                                        'amt_consolidated_revenue'])\n",
    "    print('Done calculating ratio of columns')\n",
    "        \n",
    "    print('Making dummies into counts')\n",
    "    df = column_dummies_into_counts(df, col_list=['color_credit_status','rat_pd', 'code_legal_form_group',\n",
    "                                                  'code_SBI_2_group'])\n",
    "    print('Done making dummies into counts')\n",
    "    \n",
    " \n",
    "    print('Calculating if any true ')\n",
    "    df = calculate_if_any_true(df, col_list=['is_discontinued', 'code_financial_calamity'])\n",
    "    print('Done calculating if any true ')\n",
    "    \n",
    "    print('Calculating mean of columns ')\n",
    "    df = calculate_mean_of_column(df, col_list=['amt_consolidated_operating_result', \n",
    "                                                        'amt_consolidated_revenue',\n",
    "                                                       'amt_operating_result','amt_revenue',\n",
    "                                                       'qty_employees', 'qty_issued_credit_reports',\n",
    "                                                      'score_payment_assessment' , \n",
    "                                                       'score_pd'])\n",
    "    print('Done calculating mean of columns ')\n",
    "    \n",
    "    print('Calculating sum of columns')\n",
    "    df = calculate_sum_of_column(df, col_list=['qty_address_mutations_month',\n",
    "                                                        'qty_started_names',\n",
    "                                                         'qty_stopped_names',\n",
    "                                                        'total_changeof_board_members_']) \n",
    "    print('Done calculating sum of columns')\n",
    "    \n",
    "    print('Calculating variance of columns')\n",
    "    df = calculate_variance_of_column(df, col_list=['qty_employees',\n",
    "                                                        'qty_issued_credit_reports',\n",
    "                                                         'score_payment_assessment',\n",
    "                                                        'score_pd']) \n",
    "    print('Done calculating variance of columns')\n",
    "\n",
    "\n",
    " \n",
    "    print('Dropping old columns')\n",
    "    df = drop_old_columns(df, col_list = ['date_established', 'year_consolidated_operating_result', \n",
    "                                          'year_consolidated_revenue', \n",
    "                                          'year_operating_result', 'year_qty_employees', 'year_revenue',\n",
    "                                          'is_discontinued', 'code_financial_calamity', \n",
    "                                          'amt_consolidated_operating_result', 'amt_consolidated_revenue', \n",
    "                                          'amt_operating_result','amt_revenue','qty_employees', \n",
    "                                          'qty_issued_credit_reports', 'score_payment_assessment' ,\n",
    "                                          'score_pd', 'color_credit_status','rat_pd',\n",
    "                                          'qty_address_mutations_month','qty_started_names',\n",
    "                                          'qty_stopped_names', 'total_changeof_board_members_', 'is_sole_proprietor',\n",
    "                                          'code_discontinuation','date_financial_calamity_started', \n",
    "                                          'date_financial_calamity_stopped', 'id_company_creditproxy', \n",
    "                                          'financial_calamity_outcome', 'has_increased_risk' , \n",
    "                                          'perc_credit_limit_adjustment',\n",
    "                                          'date_start', 'from_date_start', 'qty_address_mutations_total',\n",
    "                                          'code_legal_form_group', 'code_SBI_2_group', 'date_relocation_last', \n",
    "                                          'date_relocation_penultimate'])\n",
    "    print('Done dropping old columns')\n",
    "\n",
    "    print(' Deduplicating rows of original dataframe')\n",
    "    df = deduplicate_rows(df)\n",
    "    print(' Done deduplicating rows of original dataframe')\n",
    "   \n",
    "    print('Getting target of next year and adding it as a column') \n",
    "    df = replace_has_relocated_with_nextyear(df= df, next_year= next_year,\n",
    "                                   dir_prefix= 'including_scores/merged_per_year/merged_cleaned/relocation_dates')\n",
    "    print('Done getting target of next year and adding it as a column' )\n",
    "    \n",
    "    print(' Cleaning aggregated data frame ')\n",
    "    df = clean_after_aggregations(df)\n",
    "    print('Done cleaning and aggreating dataframe')\n",
    "    \n",
    "    if save_df_locally_flag:\n",
    "        print('Saving DF local to VM into files_to_bucket folder')\n",
    "        save_df_locally(df= df, dir_prefix= 'files_to_bucket/aggregated', year = year)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DF for year  2017\n",
      "Starting with year:  2017\n",
      "02_clean\n",
      "blob 02_cleaned/2013_merged.csv\n",
      "The number of rows so far is:  0\n",
      "blob 02_cleaned/2014_merged.csv\n",
      "The number of rows so far is:  0\n",
      "blob 02_cleaned/2015_merged.csv\n",
      "The number of rows so far is:  0\n",
      "blob 02_cleaned/2016_merged.csv\n",
      "The number of rows so far is:  0\n",
      "blob 02_cleaned/2017_merged.csv\n",
      "Processing file:  02_cleaned/2017_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (32,35,36,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows so far is:  10912890\n"
     ]
    }
   ],
   "source": [
    "year = '2017'\n",
    "dir_prefix = '02_clean'\n",
    "\n",
    "next_year = int(year) + 1\n",
    "print('Reading DF for year ', year) \n",
    "df = read_one_year_from_bucket_merged_csv(year, dir_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10912890"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credit_flags = df.groupby(['id_company', 'id_branch', 'color_credit_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color_credit_status</th>\n",
       "      <th>G</th>\n",
       "      <th>O</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_company</th>\n",
       "      <th>id_branch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <th>10079408</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <th>10079416</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <th>10079424</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <th>10079432</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.0</th>\n",
       "      <th>35</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.0</th>\n",
       "      <th>43</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.0</th>\n",
       "      <th>51</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23.0</th>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25.0</th>\n",
       "      <th>10079467</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47.0</th>\n",
       "      <th>140</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53.0</th>\n",
       "      <th>175</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54.0</th>\n",
       "      <th>183</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62.0</th>\n",
       "      <th>10079513</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63.0</th>\n",
       "      <th>248</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68.0</th>\n",
       "      <th>272</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69.0</th>\n",
       "      <th>280</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80.0</th>\n",
       "      <th>310</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83.0</th>\n",
       "      <th>10079548</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84.0</th>\n",
       "      <th>337</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90.0</th>\n",
       "      <th>10079556</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92.0</th>\n",
       "      <th>10079564</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98.0</th>\n",
       "      <th>10079572</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99.0</th>\n",
       "      <th>418</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.0</th>\n",
       "      <th>10079580</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103.0</th>\n",
       "      <th>10079599</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107.0</th>\n",
       "      <th>426</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117.0</th>\n",
       "      <th>485</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124.0</th>\n",
       "      <th>523</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126.0</th>\n",
       "      <th>10079610</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131.0</th>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487578.0</th>\n",
       "      <th>68307330</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487594.0</th>\n",
       "      <th>68307357</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487608.0</th>\n",
       "      <th>68307365</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487632.0</th>\n",
       "      <th>68307403</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487667.0</th>\n",
       "      <th>68307446</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487683.0</th>\n",
       "      <th>68307462</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487713.0</th>\n",
       "      <th>68307497</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487721.0</th>\n",
       "      <th>68307500</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487748.0</th>\n",
       "      <th>68307519</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487861.0</th>\n",
       "      <th>68307640</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487888.0</th>\n",
       "      <th>68307659</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487942.0</th>\n",
       "      <th>68307705</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487950.0</th>\n",
       "      <th>68307713</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487969.0</th>\n",
       "      <th>68307934</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487985.0</th>\n",
       "      <th>68307950</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939487993.0</th>\n",
       "      <th>68307969</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939488035.0</th>\n",
       "      <th>68308000</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939488051.0</th>\n",
       "      <th>68308027</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939488183.0</th>\n",
       "      <th>68308159</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939488256.0</th>\n",
       "      <th>68308213</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939488280.0</th>\n",
       "      <th>68308256</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939488302.0</th>\n",
       "      <th>68308272</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939488310.0</th>\n",
       "      <th>68308280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939488329.0</th>\n",
       "      <th>68308299</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939488337.0</th>\n",
       "      <th>68308302</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939488515.0</th>\n",
       "      <th>68308485</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939488639.0</th>\n",
       "      <th>68308604</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939489031.0</th>\n",
       "      <th>68308981</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939489139.0</th>\n",
       "      <th>68309090</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939489325.0</th>\n",
       "      <th>68309287</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1906182 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "color_credit_status    G  O  R\n",
       "id_company  id_branch         \n",
       "3.0         10079408   6  0  0\n",
       "5.0         10079416   6  0  0\n",
       "6.0         10079424   0  0  6\n",
       "9.0         10079432   6  0  0\n",
       "12.0        35         6  0  0\n",
       "14.0        43         6  0  0\n",
       "17.0        51         6  0  0\n",
       "23.0        94         0  0  6\n",
       "25.0        10079467   0  0  6\n",
       "47.0        140        6  0  0\n",
       "53.0        175        6  0  0\n",
       "54.0        183        6  0  0\n",
       "62.0        10079513   6  0  0\n",
       "63.0        248        6  0  0\n",
       "68.0        272        0  3  3\n",
       "69.0        280        6  0  0\n",
       "80.0        310        6  0  0\n",
       "83.0        10079548   6  0  0\n",
       "84.0        337        6  0  0\n",
       "90.0        10079556   0  0  6\n",
       "92.0        10079564   6  0  0\n",
       "98.0        10079572   6  0  0\n",
       "99.0        418        6  0  0\n",
       "100.0       10079580   0  0  6\n",
       "103.0       10079599   0  0  6\n",
       "107.0       426        6  0  0\n",
       "117.0       485        0  0  6\n",
       "124.0       523        6  0  0\n",
       "126.0       10079610   6  0  0\n",
       "131.0       566        0  0  6\n",
       "...                   .. .. ..\n",
       "939487578.0 68307330   1  0  0\n",
       "939487594.0 68307357   1  0  0\n",
       "939487608.0 68307365   1  0  0\n",
       "939487632.0 68307403   1  0  0\n",
       "939487667.0 68307446   0  0  1\n",
       "939487683.0 68307462   1  0  0\n",
       "939487713.0 68307497   1  0  0\n",
       "939487721.0 68307500   1  0  0\n",
       "939487748.0 68307519   0  0  1\n",
       "939487861.0 68307640   0  0  1\n",
       "939487888.0 68307659   1  0  0\n",
       "939487942.0 68307705   1  0  0\n",
       "939487950.0 68307713   1  0  0\n",
       "939487969.0 68307934   1  0  0\n",
       "939487985.0 68307950   1  0  0\n",
       "939487993.0 68307969   0  0  1\n",
       "939488035.0 68308000   1  0  0\n",
       "939488051.0 68308027   1  0  0\n",
       "939488183.0 68308159   0  0  1\n",
       "939488256.0 68308213   1  0  0\n",
       "939488280.0 68308256   1  0  0\n",
       "939488302.0 68308272   0  0  1\n",
       "939488310.0 68308280   0  0  1\n",
       "939488329.0 68308299   0  0  1\n",
       "939488337.0 68308302   0  0  1\n",
       "939488515.0 68308485   0  0  1\n",
       "939488639.0 68308604   0  0  1\n",
       "939489031.0 68308981   1  0  0\n",
       "939489139.0 68309090   1  0  0\n",
       "939489325.0 68309287   1  0  0\n",
       "\n",
       "[1906182 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_flags.size().unstack(fill_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Creating SBI groups ')\n",
    "df = create_sbi_groups(df)\n",
    "print('Done creating SBI groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Calculating delta of variables ')\n",
    "df = calculate_delta_of_column(df, col_list=['qty_employees','qty_issued_credit_reports', \n",
    "                                                    'score_payment_assessment',\n",
    "                                                   'code_legal_form', 'code_SBI_2_group'])\n",
    "print('Done calculating delta of variables ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Creating code legal form groups ')\n",
    "df = create_code_legal_form_groups(df)\n",
    "print('Done creating code legal form groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Calculating ages of variables ')\n",
    "df = calculate_age_based_on_date(df,['date_established', 'year_consolidated_operating_result', \n",
    "                                     'year_consolidated_revenue',\n",
    "                                    'year_operating_result', 'year_qty_employees', 'year_revenue', 'date_relocation_last'])\n",
    "print('Done calculating ages of variables ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Calculating ratio of columns')\n",
    "df = calculate_ratio_of_column(df, col_list=['amt_operating_result',\n",
    "                                                    'amt_consolidated_operating_result',\n",
    "                                                     'amt_revenue',\n",
    "                                                    'amt_consolidated_revenue'])\n",
    "print('Done calculating ratio of columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Making dummies into counts')\n",
    "df = column_dummies_into_counts(df, col_list=['color_credit_status','rat_pd', 'code_legal_form_group',\n",
    "                                              'code_SBI_2_group'])\n",
    "print('Done making dummies into counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Calculating if any true ')\n",
    "df = calculate_if_any_true(df, col_list=['is_discontinued', 'code_financial_calamity'])\n",
    "print('Done calculating if any true ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Calculating mean of columns ')\n",
    "df = calculate_mean_of_column(df, col_list=['amt_consolidated_operating_result', \n",
    "                                                    'amt_consolidated_revenue',\n",
    "                                                   'amt_operating_result','amt_revenue',\n",
    "                                                   'qty_employees', 'qty_issued_credit_reports',\n",
    "                                                  'score_payment_assessment' , \n",
    "                                                   'score_pd'])\n",
    "print('Done calculating mean of columns ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Calculating sum of columns')\n",
    "df = calculate_sum_of_column(df, col_list=['qty_address_mutations_month',\n",
    "                                                    'qty_started_names',\n",
    "                                                     'qty_stopped_names',\n",
    "                                                    'total_changeof_board_members_']) \n",
    "print('Done calculating sum of columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Calculating variance of columns')\n",
    "df = calculate_variance_of_column(df, col_list=['qty_employees',\n",
    "                                                    'qty_issued_credit_reports',\n",
    "                                                     'score_payment_assessment',\n",
    "                                                    'score_pd']) \n",
    "print('Done calculating variance of columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = df.groupby(['id_company', \n",
    "            'id_branch', \n",
    "            'color_credit_status']).size().unstack(fill_value=0).columns = ['qty_flags_green', \n",
    "                                                                            'qty_flags_orange', \n",
    "                                                                            'qty_flags_red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(DataFrame(df).head(5).to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Dropping old columns')\n",
    "df = drop_old_columns(df, col_list = ['date_established', 'year_consolidated_operating_result', \n",
    "                                      'year_consolidated_revenue', \n",
    "                                      'year_operating_result', 'year_qty_employees', 'year_revenue',\n",
    "                                      'is_discontinued', 'code_financial_calamity', \n",
    "                                      'amt_consolidated_operating_result', 'amt_consolidated_revenue', \n",
    "                                      'amt_operating_result','amt_revenue','qty_employees', \n",
    "                                      'qty_issued_credit_reports', 'score_payment_assessment' ,\n",
    "                                      'score_pd', 'color_credit_status','rat_pd',\n",
    "                                      'qty_address_mutations_month','qty_started_names',\n",
    "                                      'qty_stopped_names', 'total_changeof_board_members_', 'is_sole_proprietor',\n",
    "                                      'code_discontinuation','date_financial_calamity_started', \n",
    "                                      'date_financial_calamity_stopped', 'id_company_creditproxy', \n",
    "                                      'financial_calamity_outcome', 'has_increased_risk' , \n",
    "                                      'perc_credit_limit_adjustment',\n",
    "                                      'date_start', 'from_date_start', 'qty_address_mutations_total',\n",
    "                                      'code_legal_form_group', 'code_SBI_2_group', 'date_relocation_last', \n",
    "                                      'date_relocation_penultimate'])\n",
    "print('Done dropping old columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(' Deduplicating rows of original dataframe')\n",
    "df = deduplicate_rows(df)\n",
    "print(' Done deduplicating rows of original dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Getting target of next year and adding it as a column') \n",
    "df = replace_has_relocated_with_nextyear(df= df, next_year= next_year,\n",
    "                               dir_prefix= '02_cleaned')\n",
    "print('Done getting target of next year and adding it as a column' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(' Cleaning aggregated data frame ')\n",
    "df = clean_after_aggregations(df)\n",
    "print('Done cleaning and aggreating dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Saving DF local to VM into files_to_bucket folder')\n",
    "save_df_locally(df= df, dir_prefix= 'files_to_bucket/aggregated', year = year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "one_year_df = aggregate_full_year(year = '2015', dir_prefix= '02_clean',\n",
    "                                  save_df_locally_flag= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previewing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(DataFrame(one_year_df).head(100).to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(one_year_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### one_year_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graydon-moving",
   "language": "python",
   "name": "graydon-moving"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
