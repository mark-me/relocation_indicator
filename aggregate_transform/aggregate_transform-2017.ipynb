{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ignore 'dask' warning\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "from google.cloud import storage\n",
    "from pandas import DataFrame\n",
    "from IPython.display import HTML\n",
    "from google.cloud.storage import Blob\n",
    "import datalab.storage as gcs_datalab\n",
    "from datetime import date\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up constants. All required\n",
    "project = 'graydon-moving-indicator'\n",
    "bucket_name = 'graydon-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing bucket\n",
    "fs = gcsfs.GCSFileSystem(project='graydon-moving-indicator')\n",
    "gcs = storage.Client()\n",
    "bucket = gcs.get_bucket(bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-aggregation\n",
    "def create_dict_types_original_data():\n",
    "    # Setting up dictionary of column types\n",
    "    dtype={ 'id_company'  :np.float64,\n",
    "        'id_branch'    :np.int64,\n",
    "        'is_discontinued':bool,\n",
    "        'code_discontinuation': np.float64,\n",
    "        'code_financial_calamity':object,\n",
    "        'financial_calamity_outcome'   : np.float64,\n",
    "        'code_legal_form' : np.float64,\n",
    "        'qty_employees' :np.float64,\n",
    "        'year_qty_employees' :np.float64,\n",
    "        'id_company_creditproxy':object,\n",
    "        'score_payment_assessment'    : np.float64,\n",
    "        'amt_revenue'  : np.float64,\n",
    "        'year_revenue'  : np.float64,\n",
    "        'amt_operating_result'   : np.float64,\n",
    "        'year_operating_result'    :object,\n",
    "        'amt_consolidated_revenue'   : np.float64,\n",
    "        'year_consolidated_revenue'   :object,\n",
    "        'amt_consolidated_operating_result'     : np.float64,\n",
    "        'year_consolidated_operating_result'   :object,\n",
    "        'qty_issued_credit_reports' : np.float64,\n",
    "        'perc_credit_limit_adjustment' :object,\n",
    "        'color_credit_status'  :object,\n",
    "        'rat_pd'              :object,\n",
    "        'score_pd'            : np.float64,\n",
    "        'has_increased_risk'  :bool,\n",
    "        'is_sole_proprietor'   :bool,\n",
    "        'code_sbi_2'         : np.float64,\n",
    "        'qty_address_mutations_total'  :np.float64,\n",
    "        'qty_address_mutations_month'   :np.float64,\n",
    "        'has_relocated':bool,\n",
    "        'qty_started_names': np.float64,\n",
    "        'qty_stopped_names': np.float64,\n",
    "        'total_changeof_board_members_' :np.float64\n",
    "    }\n",
    "    return dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_parse_dates_list_original_data():\n",
    "    # Setting up dictionary of column types\n",
    "    parse_dates = ['date_established' , 'date_financial_calamity_started',\n",
    "        'date_financial_calamity_stopped', 'date_month', 'date_relocation_last']\n",
    "    return parse_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_one_year_from_bucket_merged_csv(year, dir_prefix = ''):\n",
    "    \"\"\" Reads a whole year of data from the already merged files \"\"\"\n",
    "    dtype = create_dict_types_original_data()\n",
    "    parse_dates = create_parse_dates_list_original_data()\n",
    "    full_year_df = pd.DataFrame()\n",
    "    print('Starting with year: ', year)\n",
    "    print(dir_prefix)\n",
    "    blob_list = list(bucket.list_blobs(prefix=dir_prefix))    \n",
    "    for blob in blob_list:  \n",
    "        print(\"blob\", blob.name)\n",
    "        if year in blob.name:\n",
    "            print('Processing file: ', blob.name)\n",
    "            with fs.open('graydon-data/' + blob.name) as f:\n",
    "                full_year_df = pd.read_csv(f, sep=',', index_col=0, dtype=dtype, parse_dates=parse_dates \n",
    "                                        )   \n",
    "        print('The number of rows so far is: ', full_year_df.shape[0])\n",
    "    return full_year_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ages of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_age_based_on_date(df, col_list):\n",
    "    df['max_date_month'] = df.groupby(['id_branch', 'id_company']).date_month.transform('max')\n",
    "    df['max_date_month_year'] = df['max_date_month'].apply(lambda x: x.year)\n",
    "    for col in col_list:\n",
    "        if col == 'date_established':\n",
    "            df['temp_date_established_year'] = df.date_established.apply(lambda x: x.year)\n",
    "            df['company_age'] = df['max_date_month_year'] - df.temp_date_established_year \n",
    "            df = df.drop(labels =['temp_date_established_year'], axis= 1)\n",
    "        elif col == 'date_relocation_last':\n",
    "            #print(df.columns)\n",
    "            df['max_date_relocation_last'] = df.groupby(['id_branch', 'id_company']).date_relocation_last.transform('max')\n",
    "            df['temp_max_date_relocation_last_year'] = df.max_date_relocation_last.apply(lambda x: x.year)\n",
    "            df['years_in_current_location'] = df['max_date_month_year'] - df.temp_max_date_relocation_last_year \n",
    "            df = df.drop(labels =['temp_max_date_relocation_last_year', 'max_date_relocation_last' ], axis= 1)\n",
    "        elif col == 'year_consolidated_operating_result':            \n",
    "            mask = (df['year_consolidated_operating_result'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_amt_consolidated_operating_result'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_amt_consolidated_operating_result'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_consolidated_operating_result.astype(float))  \n",
    "        elif col == 'year_consolidated_revenue':\n",
    "            mask = (df['year_consolidated_revenue'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_amt_consolidated_revenue'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_amt_consolidated_revenue'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_consolidated_revenue.astype(float))    \n",
    "        elif col == 'year_operating_result':\n",
    "            mask = (df['year_operating_result'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_amt_operating_result'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_amt_operating_result'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_operating_result.astype(float))    \n",
    "        elif col == 'year_qty_employees':\n",
    "            mask = (df['year_qty_employees'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_qty_employees'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_qty_employees'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_qty_employees.astype(float))  \n",
    "        elif col == 'year_revenue':\n",
    "            mask = (df['year_revenue'].astype(float) > 0)\n",
    "            df_valid = df[mask]\n",
    "            df['years_since_last_amt_revenue'] = np.nan\n",
    "            df.loc[mask, 'years_since_last_amt_revenue'] = (df['max_date_month_year'] - \n",
    "                                df_valid.year_revenue.astype(float)) \n",
    "    df = df.drop(labels =['max_date_month', 'max_date_month_year'], axis= 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_delta_of_column(df, col_list):\n",
    "    subset_columns = ['date_month', 'id_company', 'id_branch']\n",
    "    subset_columns.extend(col_list)\n",
    "    temp_df = df.reset_index().loc[:, subset_columns].sort_values(['id_company','id_branch', 'date_month'])\n",
    "    temp_df = temp_df.groupby(['id_branch', 'id_company']).agg(['first', 'last'])\n",
    "    for col in col_list:\n",
    "        if col == 'qty_employees':\n",
    "            temp_df['delta_qty_employees'] = temp_df['qty_employees']['last'] - temp_df['qty_employees']['first']    \n",
    "        elif col == 'qty_issued_credit_reports':\n",
    "            temp_df['delta_qty_issued_credit_reports'] = (temp_df['qty_issued_credit_reports']['last'] - \n",
    "                                                          temp_df['qty_issued_credit_reports']['first'] )\n",
    "        elif col == 'score_payment_assessment':\n",
    "            temp_df['delta_score_payment_assessment'] = (temp_df['score_payment_assessment']['last'] - \n",
    "                                                          temp_df['score_payment_assessment']['first'] )\n",
    "        elif col == 'score_pd':\n",
    "            temp_df['delta_score_pd'] = (temp_df['score_pd']['last'] - \n",
    "                                                          temp_df['score_pd']['first'] )\n",
    "        elif col == 'code_legal_form':\n",
    "            temp_df['code_legal_form_has_changed'] = (temp_df['code_legal_form']['last'] !=\n",
    "                                                          temp_df['code_legal_form']['first'] )\n",
    "        elif col == 'code_SBI_2_group':\n",
    "            temp_df['SBI_has_changed'] = (temp_df['code_SBI_2_group']['last'] !=\n",
    "                                                          temp_df['code_SBI_2_group']['first'] )   \n",
    "    temp_df.columns = temp_df.columns.droplevel(1)\n",
    "    temp_df = temp_df.loc[:,~temp_df.columns.duplicated()]\n",
    "    temp_df = temp_df.drop(axis=1, columns=col_list)        \n",
    "    df = df.merge(temp_df, how='left', on=['date_month', 'id_company', 'id_branch']) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If any true then true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_if_any_true(df, col_list):\n",
    "    for col in col_list:\n",
    "        if col == 'is_discontinued': \n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['is_discontinued'] \n",
    "                        .any()              # True if any items are True\n",
    "                        .rename('is_discontinued_any')    # name Series \n",
    "                        .to_frame()         # make a dataframe for merging\n",
    "                        .reset_index())\n",
    "        elif col == 'code_financial_calamity':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['code_financial_calamity'] \n",
    "                        .any()            \n",
    "                        .rename('has_financial_calamity')   \n",
    "                        .to_frame() \n",
    "                        .reset_index())\n",
    "        elif col == 'has_relocated':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['has_relocated'] \n",
    "                        .any()            \n",
    "                        .rename('has_relocated_next_year')   \n",
    "                        .to_frame() \n",
    "                        .reset_index())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_mean_of_column(df, col_list):\n",
    "    for col in col_list:\n",
    "        if col == 'amt_consolidated_operating_result':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['amt_consolidated_operating_result'] \n",
    "                        .agg('mean')             \n",
    "                        .rename('mean_amt_consolidated_operating_result')    \n",
    "                        .to_frame()       \n",
    "                        .reset_index())\n",
    "        if col == 'amt_consolidated_revenue':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['amt_consolidated_revenue'] \n",
    "                        .agg('mean')              \n",
    "                        .rename('mean_amt_consolidated_revenue')    \n",
    "                        .to_frame()      \n",
    "                        .reset_index())\n",
    "        if col == 'amt_operating_result':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['amt_operating_result'] \n",
    "                        .agg('mean')           \n",
    "                        .rename('mean_amt_operating_result')    \n",
    "                        .to_frame()         \n",
    "                        .reset_index())\n",
    "        if col == 'amt_revenue':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['amt_revenue']\n",
    "                        .agg('mean')        \n",
    "                        .rename('mean_amt_revenue')   \n",
    "                        .to_frame()     \n",
    "                        .reset_index())\n",
    "        if col == 'qty_employees':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_employees'] \n",
    "                        .agg('mean')          \n",
    "                        .rename('mean_qty_employees')    \n",
    "                        .to_frame()       \n",
    "                        .reset_index())\n",
    "        if col == 'qty_issued_credit_reports':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_issued_credit_reports'] \n",
    "                        .agg('mean')       \n",
    "                        .rename('mean_qty_issued_credit_reports')    \n",
    "                        .to_frame()        \n",
    "                        .reset_index())\n",
    "        if col == 'score_payment_assessment':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['score_payment_assessment'] \n",
    "                        .agg('mean')       \n",
    "                        .rename('mean_score_payment_assessment')    \n",
    "                        .to_frame()        \n",
    "                        .reset_index())\n",
    "        if col == 'score_pd':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['score_pd'] \n",
    "                        .agg('mean')       \n",
    "                        .rename('mean_score_pd')    \n",
    "                        .to_frame()        \n",
    "                        .reset_index())        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummies into counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def column_dummies_into_counts(df, col_list):\n",
    "    df = df.reset_index()\n",
    "    subset_columns = ['id_branch']\n",
    "    subset_columns.extend(col_list)\n",
    "    df['unique_id'] =  df['id_branch'].astype(str) + '_' + df['id_company'].astype(str)\n",
    "    for col in col_list:\n",
    "        temp_df = df.loc[:, subset_columns]\n",
    "        if col == 'color_credit_status':\n",
    "            temp_df = pd.crosstab(df['unique_id'], df['color_credit_status']).reset_index().rename_axis(None,\n",
    "                                                                                                        axis=1).rename(\n",
    "                columns={\"G\": \"qty_green_flags\", \"O\": \"qty_orange_flags\",\"R\": \"qty_red_flags\"})\n",
    "        elif col == 'rat_pd':\n",
    "            temp_df = pd.crosstab(df['unique_id'], df['rat_pd']).reset_index().rename_axis(None, axis=1)\n",
    "        elif col == 'code_SBI_2_group':\n",
    "            temp_df = pd.crosstab(df['unique_id'], df['code_SBI_2_group']).reset_index().rename_axis(None,\n",
    "                                                                                                        axis=1).rename(\n",
    "                columns={\"1\": \"SBI_group_1\", \"2\": \"SBI_group_2\"})\n",
    "        elif col == 'code_legal_form_group':\n",
    "            temp_df = pd.crosstab(df['unique_id'], df['code_legal_form_group']).reset_index().rename_axis(None,\n",
    "                                                                                                        axis=1).rename(\n",
    "                columns={\"1\": \"code_legal_form_group_1\", \"2\": \"code_legal_form_group_2\"})\n",
    "        df = df.merge(temp_df, how='left', on= ['unique_id']) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_ratio_of_column(df, col_list):\n",
    "    for col in col_list:\n",
    "        subset_columns = ['id_branch']\n",
    "        subset_columns.extend(col)\n",
    "        temp_df = df.loc[:, subset_columns]\n",
    "        if col == 'amt_operating_result':\n",
    "            temp_df = df.groupby(['id_branch', 'id_company'])\n",
    "            temp_df = temp_df.agg({'amt_operating_result': 'sum', 'amt_consolidated_operating_result': 'sum'}).rename(\n",
    "    columns={'amt_operating_result': 'sum_amt_operating_result', \n",
    "             'amt_consolidated_operating_result': 'sum_amt_consolidated_operating_result'})\n",
    "            temp_df['ratio_operating_result_consolidated_operating_result'] = np.divide(\n",
    "                temp_df['sum_amt_operating_result'], temp_df['sum_amt_consolidated_operating_result'])\n",
    "            temp_df = temp_df.reset_index()\n",
    "            temp_df = temp_df.drop(axis=1, columns=['sum_amt_consolidated_operating_result', \n",
    "                                                    'sum_amt_operating_result'])\n",
    "            df = df.merge(temp_df, how='left', on= ['id_branch', 'id_company'])  \n",
    "        elif col == 'amt_revenue':\n",
    "            temp_df = df.groupby(['id_branch', 'id_company'])\n",
    "            temp_df = temp_df.agg({'amt_revenue': 'sum', 'amt_consolidated_revenue': 'sum'}).rename(\n",
    "    columns={'amt_revenue': 'sum_amt_revenue', \n",
    "             'amt_consolidated_revenue': 'sum_amt_consolidated_revenue'})\n",
    "            temp_df['ratio_revenue_consolidated_revenue'] = np.divide(temp_df['sum_amt_revenue'],\n",
    "                                                                     temp_df['sum_amt_consolidated_revenue'])\n",
    "            temp_df = temp_df.reset_index()\n",
    "            temp_df = temp_df.drop(axis=1, columns=['sum_amt_revenue', 'sum_amt_consolidated_revenue'])\n",
    "            df = df.merge(temp_df, how='left',  on= ['id_branch', 'id_company'])  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_sum_of_column(df, col_list):\n",
    "    for col in col_list:\n",
    "        if col == 'qty_address_mutations_month':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_address_mutations_month'] \n",
    "                        .agg('sum')             \n",
    "                        .rename('qty_address_mutations_year')    \n",
    "                        .to_frame()       \n",
    "                        .reset_index())\n",
    "        elif col == 'qty_started_names':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_started_names'] \n",
    "                        .agg('sum')              \n",
    "                        .rename('qty_started_names_year')    \n",
    "                        .to_frame()      \n",
    "                        .reset_index())\n",
    "        elif col == 'qty_stopped_names':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_stopped_names'] \n",
    "                        .agg('sum')           \n",
    "                        .rename('qty_stopped_names_year')    \n",
    "                        .to_frame()         \n",
    "                        .reset_index())\n",
    "        elif col == 'total_changeof_board_members_':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['total_changeof_board_members_']\n",
    "                        .agg('sum')        \n",
    "                        .rename('qty_board_changes_year')   \n",
    "                        .to_frame()     \n",
    "                        .reset_index())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_variance_of_column(df, col_list):\n",
    "    for col in col_list:\n",
    "        if col == 'qty_employees':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_employees'] \n",
    "                        .agg('var')             \n",
    "                        .rename('variance_qty_employees')    \n",
    "                        .to_frame()       \n",
    "                        .reset_index())\n",
    "        elif col == 'qty_issued_credit_reports':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['qty_issued_credit_reports'] \n",
    "                        .agg('var')              \n",
    "                        .rename('variance_qty_issued_credit_reports')    \n",
    "                        .to_frame()      \n",
    "                        .reset_index())\n",
    "        elif col == 'score_payment_assessment':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['score_payment_assessment'] \n",
    "                        .agg('sum')           \n",
    "                        .rename('variance_score_payment_assessment')    \n",
    "                        .to_frame()         \n",
    "                        .reset_index())\n",
    "        elif col == 'score_pd':\n",
    "            df = df.merge(df.groupby(['id_branch', 'id_company'])['score_pd']\n",
    "                        .agg('sum')        \n",
    "                        .rename('variance_score_pd')   \n",
    "                        .to_frame()     \n",
    "                        .reset_index())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get has_relocated from next year DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_has_relocated_with_nextyear(df, next_year, dir_prefix = ''):\n",
    "    dtype={ \n",
    "            'id_branch'    :np.int64,\n",
    "            'id_company'    :np.int64,\n",
    "            'has_relocated':bool\n",
    "    }\n",
    "    full_next_year_df = pd.DataFrame()\n",
    "    cols = ['id_company', 'id_branch', 'has_relocated']\n",
    "    print('Starting withGra year: ', next_year)\n",
    "    print(dir_prefix)\n",
    "    blob_list = list(bucket.list_blobs(prefix=dir_prefix))    \n",
    "    for blob in blob_list:         \n",
    "        if str(next_year) in blob.name:\n",
    "            print('Processing file: ', blob.name)\n",
    "            with fs.open('graydon-data/' + blob.name) as f:\n",
    "                full_next_year_df = pd.read_csv(f, sep=',',  dtype=dtype, usecols= cols\n",
    "                                     )   \n",
    "        print('The number of rows so far is: ', full_next_year_df.shape[0])\n",
    "    full_next_year_df = calculate_if_any_true(full_next_year_df, col_list = ['has_relocated'])\n",
    "    full_next_year_df = full_next_year_df.drop(axis=1, columns='has_relocated')\n",
    "    full_next_year_df = full_next_year_df.drop_duplicates().reset_index().drop(axis=1, columns='index')\n",
    "    df = df.merge(full_next_year_df, on=['id_branch', 'id_company'], how='left', suffixes='_C')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating SBI code groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_sbi_groups(df):\n",
    "    code_SBI_2_group1 = [1,19,35,51,53,59,61,62,63,69,72,73,74,78,79,80,82,85,86,87,88,90,93,94]\n",
    "    df['code_SBI_2_group'] = np.where(df['code_sbi_2'].isin(code_SBI_2_group1), \"1\", \"2\")\n",
    "    df = df.drop(axis=1, labels='code_sbi_2', inplace=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating code legal from groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_code_legal_form_groups(df):\n",
    "    code_legal_form_group = [1,4,6,7,8,9,15,17,18]\n",
    "    df['code_legal_form_group'] = np.where(df['code_legal_form'].isin(code_legal_form_group), \"1\", \"2\")\n",
    "    df = df.drop(axis=1, labels='code_legal_form', inplace=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping old columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_old_columns(df, col_list):\n",
    "    df = df.drop(axis=1, labels=col_list, inplace=False)   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deduplicating rows of original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deduplicate_rows(df):\n",
    "    df = df.groupby(['id_branch', 'id_company']).first()\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(axis=1, columns='index')\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning after aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_after_aggregations(df):\n",
    "    df[['has_financial_calamity', 'is_discontinued_any', 'SBI_has_changed'\n",
    "        ,'code_legal_form_has_changed']] = df[['has_financial_calamity', 'is_discontinued_any',\n",
    "                                                'code_legal_form_has_changed', 'SBI_has_changed']].fillna(value=False)\n",
    "    \n",
    "    columns_to_zero = ['mean_qty_issued_credit_reports', 'qty_green_flags', 'qty_orange_flags', \n",
    "                       'qty_red_flags', 'AAA', 'AA', 'A', 'BBB', 'B' , 'CCC', 'CC', 'C', 'D', \n",
    "                       'NR', 'qty_address_mutations_year', 'qty_started_names_year', \n",
    "                       'qty_stopped_names_year', 'qty_board_changes_year', 'code_legal_form_group_1', \n",
    "                       'code_legal_form_group_2', 'SBI_group_1', 'SBI_group_2']\n",
    "    \n",
    "    df[columns_to_zero] = df[columns_to_zero].fillna(value=0)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving DF locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_locally(df, dir_prefix, year, as_json= False):\n",
    "    \"\"\" Saves df as json or csv locally on server \"\"\"\n",
    "    if as_json:        \n",
    "        file_path = dir_prefix + '/' + year + '_aggregated.json'\n",
    "        df.to_json(file_path)\n",
    "    else:\n",
    "        file_path =  dir_prefix + '/' + year + '_aggregated.csv'\n",
    "        df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating dataframe into one year. Main function that calls them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregating dataframe into one year. Main function that calls them all\n",
    "def aggregate_full_year(year, dir_prefix = '', save_df_locally_flag = False):\n",
    "    \n",
    "    next_year = int(year) + 1\n",
    "    \n",
    "    print('Reading DF for year ', year) \n",
    "    df = read_one_year_from_bucket_merged_csv(year, dir_prefix)\n",
    "    \n",
    "    print('Creating SBI groups ')\n",
    "    df = create_sbi_groups(df)\n",
    "    print('Done creating SBI groups')\n",
    "    \n",
    "    print('Calculating delta of variables ')\n",
    "    df = calculate_delta_of_column(df, col_list=['qty_employees','qty_issued_credit_reports', \n",
    "                                                        'score_payment_assessment',\n",
    "                                                       'code_legal_form', 'code_SBI_2_group'])\n",
    "    print('Done calculating delta of variables ') \n",
    "    \n",
    "    print('Creating code legal form groups ')\n",
    "    df = create_code_legal_form_groups(df)\n",
    "    print('Done creating code legal form groups')\n",
    "    \n",
    "    print('Calculating ages of variables ')\n",
    "    df = calculate_age_based_on_date(df,['date_established', 'year_consolidated_operating_result', \n",
    "                                         'year_consolidated_revenue',\n",
    "                                        'year_operating_result', 'year_qty_employees', 'year_revenue', 'date_relocation_last'])\n",
    "    print('Done calculating ages of variables ')\n",
    "    \n",
    "    print('Calculating ratio of columns')\n",
    "    df = calculate_ratio_of_column(df, col_list=['amt_operating_result',\n",
    "                                                        'amt_consolidated_operating_result',\n",
    "                                                         'amt_revenue',\n",
    "                                                        'amt_consolidated_revenue'])\n",
    "    print('Done calculating ratio of columns')\n",
    "        \n",
    "    print('Making dummies into counts')\n",
    "    df = column_dummies_into_counts(df, col_list=['color_credit_status','rat_pd', 'code_legal_form_group',\n",
    "                                                  'code_SBI_2_group'])\n",
    "    print('Done making dummies into counts')\n",
    "    \n",
    " \n",
    "    print('Calculating if any true ')\n",
    "    df = calculate_if_any_true(df, col_list=['is_discontinued', 'code_financial_calamity'])\n",
    "    print('Done calculating if any true ')\n",
    "    \n",
    "    print('Calculating mean of columns ')\n",
    "    df = calculate_mean_of_column(df, col_list=['amt_consolidated_operating_result', \n",
    "                                                        'amt_consolidated_revenue',\n",
    "                                                       'amt_operating_result','amt_revenue',\n",
    "                                                       'qty_employees', 'qty_issued_credit_reports',\n",
    "                                                      'score_payment_assessment' , \n",
    "                                                       'score_pd'])\n",
    "    print('Done calculating mean of columns ')\n",
    "    \n",
    "    print('Calculating sum of columns')\n",
    "    df = calculate_sum_of_column(df, col_list=['qty_address_mutations_month',\n",
    "                                                        'qty_started_names',\n",
    "                                                         'qty_stopped_names',\n",
    "                                                        'total_changeof_board_members_']) \n",
    "    print('Done calculating sum of columns')\n",
    "    \n",
    "    print('Calculating variance of columns')\n",
    "    df = calculate_variance_of_column(df, col_list=['qty_employees',\n",
    "                                                        'qty_issued_credit_reports',\n",
    "                                                         'score_payment_assessment',\n",
    "                                                        'score_pd']) \n",
    "    print('Done calculating variance of columns')\n",
    "\n",
    "\n",
    " \n",
    "    print('Dropping old columns')\n",
    "    df = drop_old_columns(df, col_list = ['date_established', 'year_consolidated_operating_result', \n",
    "                                          'year_consolidated_revenue', \n",
    "                                          'year_operating_result', 'year_qty_employees', 'year_revenue',\n",
    "                                          'is_discontinued', 'code_financial_calamity', \n",
    "                                          'amt_consolidated_operating_result', 'amt_consolidated_revenue', \n",
    "                                          'amt_operating_result','amt_revenue','qty_employees', \n",
    "                                          'qty_issued_credit_reports', 'score_payment_assessment' ,\n",
    "                                          'score_pd', 'color_credit_status','rat_pd',\n",
    "                                          'qty_address_mutations_month','qty_started_names',\n",
    "                                          'qty_stopped_names', 'total_changeof_board_members_', 'is_sole_proprietor',\n",
    "                                          'code_discontinuation','date_financial_calamity_started', \n",
    "                                          'date_financial_calamity_stopped', 'id_company_creditproxy', \n",
    "                                          'financial_calamity_outcome', 'has_increased_risk' , \n",
    "                                          'perc_credit_limit_adjustment',\n",
    "                                          'date_start', 'from_date_start', 'qty_address_mutations_total',\n",
    "                                          'code_legal_form_group', 'code_SBI_2_group', 'date_relocation_last', \n",
    "                                          'date_relocation_penultimate'])\n",
    "    print('Done dropping old columns')\n",
    "\n",
    "    print(' Deduplicating rows of original dataframe')\n",
    "    df = deduplicate_rows(df)\n",
    "    print(' Done deduplicating rows of original dataframe')\n",
    "   \n",
    "    print('Getting target of next year and adding it as a column') \n",
    "    df = replace_has_relocated_with_nextyear(df= df, next_year= next_year,\n",
    "                                   dir_prefix= '02_cleaned')\n",
    "    print('Done getting target of next year and adding it as a column' )\n",
    "    \n",
    "    print(' Cleaning aggregated data frame ')\n",
    "    df = clean_after_aggregations(df)\n",
    "    print('Done cleaning and aggreating dataframe')\n",
    "    \n",
    "    if save_df_locally_flag:\n",
    "        print('Saving DF local to VM into files_to_bucket folder')\n",
    "        save_df_locally(df= df, dir_prefix= 'files_to_bucket/aggregated', year = year)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DF for year  2017\n",
      "Starting with year:  2017\n",
      "02_clean\n",
      "blob 02_cleaned/2013_merged.csv\n",
      "The number of rows so far is:  0\n",
      "blob 02_cleaned/2014_merged.csv\n",
      "The number of rows so far is:  0\n",
      "blob 02_cleaned/2015_merged.csv\n",
      "The number of rows so far is:  0\n",
      "blob 02_cleaned/2016_merged.csv\n",
      "The number of rows so far is:  0\n",
      "blob 02_cleaned/2017_merged.csv\n",
      "Processing file:  02_cleaned/2017_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/IPython/core/magics/execution.py:1246: DtypeWarning: Columns (32,35,36,43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code, glob, local_ns)\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows so far is:  22729762\n",
      "Creating SBI groups \n",
      "Done creating SBI groups\n",
      "Calculating delta of variables \n",
      "Done calculating delta of variables \n",
      "Creating code legal form groups \n",
      "Done creating code legal form groups\n",
      "Calculating ages of variables \n",
      "Done calculating ages of variables \n",
      "Calculating ratio of columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: overflow encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done calculating ratio of columns\n",
      "Making dummies into counts\n",
      "Done making dummies into counts\n",
      "Calculating if any true \n",
      "Done calculating if any true \n",
      "Calculating mean of columns \n",
      "Done calculating mean of columns \n",
      "Calculating sum of columns\n",
      "Done calculating sum of columns\n",
      "Calculating variance of columns\n",
      "Done calculating variance of columns\n",
      "Dropping old columns\n",
      "Done dropping old columns\n",
      " Deduplicating rows of original dataframe\n",
      " Done deduplicating rows of original dataframe\n",
      "Getting target of next year and adding it as a column\n",
      "Starting withGra year:  2018\n",
      "02_cleaned\n",
      "The number of rows so far is:  0\n",
      "The number of rows so far is:  0\n",
      "The number of rows so far is:  0\n",
      "The number of rows so far is:  0\n",
      "The number of rows so far is:  0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id_branch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-b8d96ba592fa>\u001b[0m in \u001b[0;36maggregate_full_year\u001b[0;34m(year, dir_prefix, save_df_locally_flag)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Getting target of next year and adding it as a column'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     df = replace_has_relocated_with_nextyear(df= df, next_year= next_year,\n\u001b[0;32m---> 97\u001b[0;31m                                    dir_prefix= '02_cleaned')\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done getting target of next year and adding it as a column'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-e6843064e4f6>\u001b[0m in \u001b[0;36mreplace_has_relocated_with_nextyear\u001b[0;34m(df, next_year, dir_prefix)\u001b[0m\n\u001b[1;32m     17\u001b[0m                                      )   \n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The number of rows so far is: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_next_year_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mfull_next_year_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_if_any_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_next_year_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'has_relocated'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mfull_next_year_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_next_year_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'has_relocated'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mfull_next_year_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_next_year_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-eeac81fa0e5e>\u001b[0m in \u001b[0;36mcalculate_if_any_true\u001b[0;34m(df, col_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m                         .reset_index())\n\u001b[1;32m     15\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'has_relocated'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             df = df.merge(df.groupby(['id_branch', 'id_company'])['has_relocated'] \n\u001b[0m\u001b[1;32m     17\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'has_relocated_next_year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m   6663\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n\u001b[1;32m   6664\u001b[0m                        \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6665\u001b[0;31m                        observed=observed, **kwargs)\n\u001b[0m\u001b[1;32m   6666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6667\u001b[0m     def asfreq(self, freq, method=None, how=None, normalize=False,\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(obj, by, **kwds)\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid type: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                                                     \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                     \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                                                     mutated=self.mutated)\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate)\u001b[0m\n\u001b[1;32m   3289\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3290\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3291\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3292\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3293\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id_branch'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "one_year_df = aggregate_full_year(year = '2017', dir_prefix= '02_clean',\n",
    "                                  save_df_locally_flag= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previewing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(DataFrame(one_year_df).head(100).to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(one_year_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### one_year_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graydon-moving",
   "language": "python",
   "name": "graydon-moving"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
