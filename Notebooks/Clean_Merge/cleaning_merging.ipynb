{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cleaning_merging import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_dataset = datetime.strptime(\"2018-01-01\", '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dates = [\"2018-01-01\", \"2017-01-01\", \"2016-01-01\", \"2015-01-01\", \"2014-01-01\", \"2013-01-01\"]\n",
    "list_date_dataset = [datetime.strptime(date, '%Y-%m-%d') for date in dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project = 'graydon-moving-indicator'\n",
    "bucket_name = 'graydon-data'\n",
    "dir_input_data = '01_input'\n",
    "dir_output_data = '02_cleaned'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate clean and merge object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_merge = Cleaner_Merger(project, bucket_name, dir_input_data, dir_output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 01_input/2017/modelling_2017-01-01_2017-01-31.CSV with 5000 rows and 57 columns\n",
      "After removing sole proprietors there are 4411 rows are left\n",
      "The number of rows so far by adding 01_input/2017/modelling_2017-01-01_2017-01-31.CSV : 4411\n",
      "Read 01_input/2017/modelling_2017-02-01_2017-02-28.CSV with 5000 rows and 57 columns\n",
      "After removing sole proprietors there are 4412 rows are left\n",
      "The number of rows so far by adding 01_input/2017/modelling_2017-02-01_2017-02-28.CSV : 8823\n",
      "Read 01_input/2017/modelling_2017-03-01_2017-03-31.CSV with 5000 rows and 57 columns\n",
      "After removing sole proprietors there are 4414 rows are left\n",
      "The number of rows so far by adding 01_input/2017/modelling_2017-03-01_2017-03-31.CSV : 13237\n",
      "Read 01_input/2017/modelling_2017-04-01_2017-04-30.CSV with 5000 rows and 57 columns\n",
      "After removing sole proprietors there are 4414 rows are left\n",
      "The number of rows so far by adding 01_input/2017/modelling_2017-04-01_2017-04-30.CSV : 17651\n",
      "Read 01_input/2017/modelling_2017-05-01_2017-05-31.CSV with 5000 rows and 57 columns\n",
      "After removing sole proprietors there are 4415 rows are left\n",
      "The number of rows so far by adding 01_input/2017/modelling_2017-05-01_2017-05-31.CSV : 22066\n",
      "Read 01_input/2017/modelling_2017-06-01_2017-06-30.CSV with 5000 rows and 57 columns\n",
      "After removing sole proprietors there are 4416 rows are left\n",
      "The number of rows so far by adding 01_input/2017/modelling_2017-06-01_2017-06-30.CSV : 26482\n",
      "Read 01_input/2017/modelling_2017-07-01_2017-07-31.CSV with 5000 rows and 57 columns\n",
      "After removing sole proprietors there are 4418 rows are left\n",
      "The number of rows so far by adding 01_input/2017/modelling_2017-07-01_2017-07-31.CSV : 30900\n",
      "Read 01_input/2017/modelling_2017-08-01_2017-08-31.CSV with 5000 rows and 57 columns\n",
      "After removing sole proprietors there are 4418 rows are left\n",
      "The number of rows so far by adding 01_input/2017/modelling_2017-08-01_2017-08-31.CSV : 35318\n",
      "Read 01_input/2017/modelling_2017-09-01_2017-09-30.CSV with 5000 rows and 57 columns\n",
      "After removing sole proprietors there are 4420 rows are left\n",
      "The number of rows so far by adding 01_input/2017/modelling_2017-09-01_2017-09-30.CSV : 39738\n",
      "Read 01_input/2017/modelling_2017-10-01_2017-10-31.CSV with 5000 rows and 57 columns\n",
      "After removing sole proprietors there are 4418 rows are left\n",
      "The number of rows so far by adding 01_input/2017/modelling_2017-10-01_2017-10-31.CSV : 44156\n",
      "Read 01_input/2017/modelling_2017-11-01_2017-11-30.CSV with 5000 rows and 57 columns\n",
      "After removing sole proprietors there are 4419 rows are left\n",
      "The number of rows so far by adding 01_input/2017/modelling_2017-11-01_2017-11-30.CSV : 48575\n",
      "Read 01_input/2017/modelling_2017-12-01_2017-12-31.CSV with 5000 rows and 57 columns\n",
      "After removing sole proprietors there are 4414 rows are left\n",
      "The number of rows so far by adding 01_input/2017/modelling_2017-12-01_2017-12-31.CSV : 52989\n",
      "Read relocation data with 8312415 rows and 4 columns.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unorderable types: int() > str()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-04280ba4af9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_merge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/graydon_moving_indicator/Notebooks/modules/cleaning_merging.py\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(self, date_month, qty_months_horizon)\u001b[0m\n\u001b[1;32m    165\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The number of rows so far by adding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_months_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m       \u001b[0mdf_months_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_previous_relocation_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_months_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m       \u001b[0mdf_months_combined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_dataset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_month\u001b[0m \u001b[0;31m# Add the identifier for a data-set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/graydon_moving_indicator/Notebooks/modules/cleaning_merging.py\u001b[0m in \u001b[0;36madd_previous_relocation_dates\u001b[0;34m(self, df_year_months)\u001b[0m\n\u001b[1;32m    125\u001b[0m                                                 how='left')\n\u001b[1;32m    126\u001b[0m       \u001b[0;31m# Removing all relocation dates after the current month\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m       \u001b[0mdf_branch_months\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_branch_months\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_branch_months\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_month'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mdf_branch_months\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_relocation_last'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m       \u001b[0;31m# Getting the latest relocation dates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0mdf_max_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_branch_months\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_company'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'id_branch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date_month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_relocation_last'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date_relocation_penultimate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m             res_values = dispatch_to_index_op(op, self, other,\n\u001b[0;32m-> 1230\u001b[0;31m                                               pd.DatetimeIndex)\n\u001b[0m\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m             return self._constructor(res_values, index=self.index,\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mdispatch_to_index_op\u001b[0;34m(op, left, right, index_class)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0mleft_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shallow_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNullFrequencyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# DatetimeIndex and TimedeltaIndex with freq == None raise ValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_tzawareness_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mcmp_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# technically we could support bool dtyped Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unorderable types: int() > str()"
     ]
    }
   ],
   "source": [
    "df_features = clean_merge.get_features(date_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of company, branch, month combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_target = clean_merge.get_targets(date_dataset, columns_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of companies with target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_target.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_monthly = df_features.merge(df_target,\n",
    "                               on=['id_company', 'id_branch'],\n",
    "                               how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_monthly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of monthly company branch data records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_monthly.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_monthly['has_relocated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_month = date_dataset\n",
    "\n",
    "df_months_combined = pd.DataFrame()  # The data frame which will contain all independent variables\n",
    "    \n",
    "# Get all months in range\n",
    "df_date_months = pd.DataFrame(pd.date_range(date_month, periods=12, freq=\"M\").tolist(),\n",
    "                              columns=['date_month'])\n",
    "df_date_months['date_month'] = df_date_months['date_month'].values.astype('datetime64[M]') # First day of month\n",
    "\n",
    "# Get the file names of all required month files\n",
    "#month_files = get_month_filenames(df_date_months, bucket, dir_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "month_files = [] # List of month files\n",
    "\n",
    "df_date_months['year'] = df_date_months.date_month.dt.year\n",
    "list_years = df_date_months['year'].unique()\n",
    "# If there are multiple years, iterate through years  \n",
    "for year in list_years:\n",
    "    # Get the year's data file names\n",
    "    dir_data_year = dir_data + '/' + str(year)\n",
    "    list_blob = list(bucket.list_blobs(prefix=dir_data_year))\n",
    "    # finding out which month files should be processed by looking which contain the first month date (YYYY-mm-01)\n",
    "    df_year_months = df_date_months[df_date_months['year'] == year]['date_month']\n",
    "    for blob in list_blob:\n",
    "        for month in df_year_months:\n",
    "            if (month.strftime(\"%Y-%m-%d\") in blob.name) & ('CSV' in blob.name):\n",
    "                month_files.append(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "month_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleaning, transforming and combining month files                \n",
    "for month_file in month_files:\n",
    "    with fs.open('graydon-data/' + month_file) as f:\n",
    "        df_month = pd.read_csv(f, sep=';', usecols= columns_features, index_col=False)   \n",
    "        df_month = df_month[(df_month['is_sole_proprietor'] == 0)] # & (one_month_df['is_discontinued'] == 0) \n",
    "        df_month.columns = (df_month.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', ''))\n",
    "        df_months_combined = df_months_combined.append(df_month)\n",
    "        print('The number of rows so far by adding ', month_file, \":\", df_months_combined.shape[0])\n",
    "\n",
    "df_months_combined['date_dataset'] = date_month\n",
    "\n",
    "# Aggregating data to year\n",
    "df_months_combined = df_months_combined.groupby(['date_dataset', \n",
    "                                                  'id_company', \n",
    "                                                  'id_branch']).agg({'has_relocated': 'max', \n",
    "                                                                     'date_month': 'max'})\n",
    "df_months_combined = df_months_combined.rename(index=str, columns={\"date_month\": \"date_month_last\"})\n",
    "df_months_combined = df_months_combined.reset_index()\n",
    "df_months_combined['date_dataset'] = pd.to_datetime(df_months_combined['date_dataset'])\n",
    "df_months_combined['id_company'] = df_months_combined['id_company'].astype(int)\n",
    "df_months_combined['id_branch'] = df_months_combined['id_branch'].astype(int)\n",
    "\n",
    "return(df_months_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graydon-moving",
   "language": "python",
   "name": "graydon-moving"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
