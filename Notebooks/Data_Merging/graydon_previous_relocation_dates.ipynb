{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ignore 'dask' warning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gcsfs\n",
    "from google.cloud import storage\n",
    "from pandas import DataFrame\n",
    "from IPython.display import HTML\n",
    "from google.cloud.storage import Blob\n",
    "import datalab.storage as gcs_datalab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up constants\n",
    "\n",
    "All required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project = 'graydon-moving-indicator'\n",
    "bucket_name = 'graydon-data'\n",
    "dir_year_files_from = 'including_scores/merged_per_year/merged_cleaned'\n",
    "dir_year_files_to = 'files_to_bucket'\n",
    "years = ['2018', '2017', '2016', '2015', '2014', '2013', '2012', '2011', '2010', '2009', '2008']\n",
    "selected_columns_small = ['date_month', 'id_company', 'id_branch', 'date_start', 'from_date_start']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(project='graydon-moving-indicator')\n",
    "gcs = storage.Client()\n",
    "bucket = gcs.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_one_year_from_bucket_merged_csv(year, dir_prefix = '', selected_columns = ''):\n",
    "    \"\"\" Reads a whole year of data from the already merged files \"\"\"\n",
    "    full_year_df = pd.DataFrame()\n",
    "    \n",
    "    # Setting up dictionary of column types\n",
    "    dtype={'id_company'  :np.float64,\n",
    "           'id_branch'    :np.int64,\n",
    "           'is_discontinued':bool,\n",
    "           'code_discontinuation': np.float64,\n",
    "           'code_financial_calamity':object,\n",
    "           'financial_calamity_outcome'   : np.float64,\n",
    "           'code_legal_form' : np.float64,\n",
    "           'qty_employees' :np.float64,\n",
    "           'year_qty_employees' :np.float64,\n",
    "           'id_company_creditproxy':object,\n",
    "           'score_payment_assessment'    : np.float64,\n",
    "           'amt_revenue'  : np.float64,\n",
    "           'year_revenue'  : np.float64,\n",
    "           'amt_operating_result'   : np.float64,\n",
    "           'year_operating_result'    :object,\n",
    "           'amt_consolidated_revenue'   : np.float64,\n",
    "           'year_consolidated_revenue'   :object,\n",
    "           'amt_consolidated_operating_result'     : np.float64,\n",
    "           'year_consolidated_operating_result'   :object,\n",
    "           'qty_issued_credit_reports' : np.float64,\n",
    "           'perc_credit_limit_adjustment' :object,\n",
    "           'color_credit_status'  :object,\n",
    "           'rat_pd'              :object,\n",
    "           'score_pd'            : np.float64,\n",
    "           'has_increased_risk'  :bool,\n",
    "           'is_sole_proprietor'   :bool,\n",
    "           'code_sbi_2'         : np.float64,\n",
    "           'code_sbi_1'          :object,\n",
    "           'qty_address_mutations_total'  :np.float64,\n",
    "           'qty_address_mutations_month'   :np.float64,\n",
    "           'has_relocated':bool,\n",
    "           'qty_started_names': np.float64,\n",
    "           'qty_stopped_names': np.float64,\n",
    "           'has_name_change':bool,\n",
    "           'total_changeof_board_members_' :np.float64\n",
    "         }\n",
    "    \n",
    "    parse_dates= ['date_established' ,'date_established', 'date_financial_calamity_started',\n",
    "           'date_financial_calamity_stopped', 'date_start', 'from_date_start' ]\n",
    "    \n",
    "    blob_list = list(bucket.list_blobs(prefix=dir_prefix))    \n",
    "    for blob in blob_list:  \n",
    "        if year in blob.name:\n",
    "            with fs.open('graydon-data/' + blob.name) as f:\n",
    "                if selected_columns == '' or None:\n",
    "                    full_year_df = pd.read_csv(f, sep=',', index_col=0, dtype=dtype, parse_dates=parse_dates) \n",
    "                else:\n",
    "                    full_year_df = pd.read_csv(f, sep=',', index_col=0, \n",
    "                                               usecols = selected_columns, dtype=dtype, parse_dates=parse_dates) \n",
    "            print('The number of rows read: ', full_year_df.shape[0])\n",
    "    return full_year_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_combined_years(year, dir_year_files, selected_columns = None):\n",
    "\n",
    "    df_all_years = pd.DataFrame()\n",
    "\n",
    "    for year in years:\n",
    "        df_one_year = read_one_year_from_bucket_merged_csv(year = year, \n",
    "                                                          dir_prefix = dir_year_files,\n",
    "                                                          selected_columns = selected_columns)\n",
    "        df_all_years = df_all_years.append(df_one_year)\n",
    "\n",
    "    return df_all_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_locally(df, dir_prefix, year, as_json= False):\n",
    "    \"\"\" Saves df as json or csv locally on server \"\"\"\n",
    "    if as_json:        \n",
    "        file_path = dir_prefix + '/' + year + '_merged.json'\n",
    "        df.to_json(file_path)\n",
    "    else:\n",
    "        file_path =  dir_prefix + '/' + year + '_merged.csv'\n",
    "        df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading relocation dates\n",
    "blob_list = list(bucket.list_blobs(prefix='location_start_date.CSV'))\n",
    "\n",
    "for blob in blob_list: \n",
    "    with fs.open('graydon-data/' + blob.name) as f:\n",
    "        df_relocation_dates = pd.read_csv(f, sep=',', \n",
    "                                          na_values=['', '1198-06-12', 'NA']) \n",
    "        df_relocation_dates['date_relocation_last'] = pd.to_datetime(df_relocation_dates['date_relocation_last'])\n",
    "        df_relocation_dates['date_relocation_penultimate'] = pd.to_datetime(df_relocation_dates['date_relocation_penultimate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preview of the data \n",
    "HTML(DataFrame(df_relocation_dates).head(5).to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#i_year = '2018'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "df_year = read_one_year_from_bucket_merged_csv(year = i_year, dir_prefix = dir_year_files_from)\n",
    "qty_rows_input = len(df_year) \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use only the relocations of i_year and the years before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "is_same_or_before_year = df_relocation_dates['date_relocation_last'].dt.year <= int(i_year)\n",
    "df_relocation_dates_year = df_relocation_dates[is_same_or_before_year]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding relocation dates for each branch month combination before the maximum date before the month date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "df_branch_months = df_year[['id_company', 'id_branch', 'date_month']]\n",
    "df_branch_months = df_branch_months.merge(df_relocation_dates_year, \n",
    "                                          on=['id_company', 'id_branch'], \n",
    "                                          how='left')\n",
    "df_max_dates = df_branch_months.groupby(['id_company', 'id_branch', 'date_month'])['date_relocation_last', 'date_relocation_penultimate'].max()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the new data to the original year data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "df_year = df_year.merge(df_max_dates,\n",
    "                        on=['id_company', 'id_branch', 'date_month'], \n",
    "                        how='left')\n",
    "qty_rows_output = len(df_year)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether number of rows yearly should remain constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "if(qty_rows_input != qty_rows_output):\n",
    "    print('Mismatch in rows for ', i_year)\n",
    "    print(\"Going in : \", qty_rows_input)\n",
    "    print(\"Going out: \", qty_rows_output)\n",
    "    raise Exception('Mismatch in rows for: {}'.format(i_year))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save_df_locally(df= df_year, dir_prefix= dir_year_files_to, year= i_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In loop form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file:  including_scores/merged_per_year/merged_cleaned/2011_merged.csv\n",
      "The number of rows read:  19853262\n",
      "2. Selecting relocation data of  2011  and before\n",
      "3. Getting relocation data for each month  2011  per branch\n",
      "4. Adding relocation dates to company data of  2011\n",
      "5. Saving enriched yearly data of  2011  locally\n",
      "6. Done processing and saving 2011\n",
      "Processing file:  including_scores/merged_per_year/merged_cleaned/2010_merged.csv\n",
      "The number of rows read:  19121202\n",
      "2. Selecting relocation data of  2010  and before\n",
      "3. Getting relocation data for each month  2010  per branch\n",
      "4. Adding relocation dates to company data of  2010\n",
      "5. Saving enriched yearly data of  2010  locally\n",
      "6. Done processing and saving 2010\n",
      "Processing file:  including_scores/merged_per_year/merged_cleaned/2009_merged.csv\n",
      "The number of rows read:  17898540\n",
      "2. Selecting relocation data of  2009  and before\n",
      "3. Getting relocation data for each month  2009  per branch\n",
      "4. Adding relocation dates to company data of  2009\n",
      "5. Saving enriched yearly data of  2009  locally\n",
      "6. Done processing and saving 2009\n",
      "Processing file:  including_scores/merged_per_year/merged_cleaned/2008_merged.csv\n",
      "The number of rows read:  17266521\n",
      "2. Selecting relocation data of  2008  and before\n",
      "3. Getting relocation data for each month  2008  per branch\n",
      "4. Adding relocation dates to company data of  2008\n",
      "5. Saving enriched yearly data of  2008  locally\n",
      "6. Done processing and saving 2008\n"
     ]
    }
   ],
   "source": [
    "for i_year in years:\n",
    "    \n",
    "    # Reading year\n",
    "    print('1. Reading monthly branch data of ', i_year)\n",
    "    df_year = read_one_year_from_bucket_merged_csv(year = i_year, dir_prefix = dir_year_files_from)\n",
    "    qty_rows_input = len(df_year) \n",
    "    \n",
    "    # Use only the relocations of i_year and the years before\n",
    "    print('2. Selecting relocation data of ', i_year, \" and before\")\n",
    "    is_same_or_before_year = df_relocation_dates['date_relocation_last'].dt.year <= int(i_year)\n",
    "    df_relocation_dates_year = df_relocation_dates[is_same_or_before_year]\n",
    "    \n",
    "    # Getting relocation dates for each branch month combination before the maximum date before the month date\n",
    "    print('3. Getting relocation data for each month ', i_year, \" per branch\")\n",
    "    df_branch_months = df_year[['id_company', 'id_branch', 'date_month']]\n",
    "    df_branch_months = df_branch_months.merge(df_relocation_dates_year, \n",
    "                                              on=['id_company', 'id_branch'], \n",
    "                                              how='left')\n",
    "    df_max_dates = df_branch_months.groupby(['id_company', 'id_branch', 'date_month'])['date_relocation_last', 'date_relocation_penultimate'].max()\n",
    "    \n",
    "    # Adding the new data to the original year data\n",
    "    print('4. Adding relocation dates to company data of ', i_year)\n",
    "    df_year = df_year.merge(df_max_dates,\n",
    "                            on=['id_company', 'id_branch', 'date_month'], \n",
    "                            how='left')\n",
    "    qty_rows_output = len(df_year)\n",
    "    \n",
    "    # Check whether number of rows yearly should remain constant\n",
    "    if(qty_rows_input != qty_rows_output):\n",
    "        print('Mismatch in rows for ', i_year)\n",
    "        print(\"Going in : \", qty_rows_input)\n",
    "        print(\"Going out: \", qty_rows_output)\n",
    "        raise Exception('Mismatch in rows for: {}'.format(i_year))\n",
    "        \n",
    "    print('5. Saving enriched yearly data of ', i_year, ' locally' )       \n",
    "    save_df_locally(df= df_year, dir_prefix= dir_year_files_to, year= i_year)\n",
    "    \n",
    "    print('6. Done processing and saving', i_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graydon-moving",
   "language": "python",
   "name": "graydon-moving"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
