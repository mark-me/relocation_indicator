{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ignore 'dask' warning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gcsfs\n",
    "from google.cloud import storage\n",
    "from pandas import DataFrame\n",
    "from IPython.display import HTML\n",
    "from google.cloud.storage import Blob\n",
    "import datalab.storage as gcs_datalab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up constants\n",
    "\n",
    "All required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project = 'graydon-moving-indicator'\n",
    "bucket_name = 'graydon-data'\n",
    "dir_year_files_from = 'including_scores/merged_per_year/aggregated/new/small'\n",
    "dir_year_files_to = 'files_to_bucket'\n",
    "years = ['2016', '2015']\n",
    "selected_columns_small = ['date_month', 'id_company', 'id_branch', 'date_start', 'from_date_start']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(project='graydon-moving-indicator')\n",
    "gcs = storage.Client()\n",
    "bucket = gcs.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_one_year_from_bucket_merged_csv(year, dir_prefix = '', selected_columns = ''):\n",
    "    \"\"\" Reads a whole year of data from the already merged files \"\"\"\n",
    "    full_year_df = pd.DataFrame()\n",
    "    \n",
    "    # Setting up dictionary of column types\n",
    "    dtype={'id_company'  :np.float64,\n",
    "           'id_branch'    :np.int64,\n",
    "           'is_discontinued':bool,\n",
    "           'code_discontinuation': np.float64,\n",
    "           'code_financial_calamity':object,\n",
    "           'financial_calamity_outcome'   : np.float64,\n",
    "           'code_legal_form' : np.float64,\n",
    "           #'qty_employees' :np.float64,\n",
    "           #'year_qty_employees' :np.float64,\n",
    "           'id_company_creditproxy':object,\n",
    "           'score_payment_assessment'    : np.float64,\n",
    "           #'amt_revenue'  : np.float64,\n",
    "           'year_revenue'  : np.float64,\n",
    "           #'amt_operating_result'   : np.float64,\n",
    "           #'year_operating_result'    :object,\n",
    "           #'amt_consolidated_revenue'   : np.float64,\n",
    "           #'year_consolidated_revenue'   :object,\n",
    "           #'amt_consolidated_operating_result'     : np.float64,\n",
    "           #'year_consolidated_operating_result'   :object,\n",
    "           'qty_issued_credit_reports' : np.float64,\n",
    "           'perc_credit_limit_adjustment' :object,\n",
    "           'color_credit_status'  :object,\n",
    "           'rat_pd'              :object,\n",
    "           #'score_pd'            : np.float64,\n",
    "           'has_increased_risk'  :bool,\n",
    "           'is_sole_proprietor'   :bool,\n",
    "           'code_sbi_2'         : np.float64,\n",
    "           'code_sbi_1'          :object,\n",
    "           'qty_address_mutations_total'  :np.float64,\n",
    "           'qty_address_mutations_month'   :np.float64,\n",
    "           'has_relocated':bool,\n",
    "           'qty_started_names': np.float64,\n",
    "           'qty_stopped_names': np.float64,\n",
    "           'has_name_change':bool,\n",
    "           'total_changeof_board_members_' :np.float64\n",
    "         }\n",
    "    \n",
    "    parse_dates= ['date_established' ,'date_established', 'date_financial_calamity_started',\n",
    "           'date_financial_calamity_stopped', 'date_start', 'from_date_start' ]\n",
    "    \n",
    "    blob_list = list(bucket.list_blobs(prefix=dir_prefix))    \n",
    "    for blob in blob_list:  \n",
    "        if year in blob.name:\n",
    "            with fs.open('graydon-data/' + blob.name) as f:\n",
    "                if selected_columns == '' or None:\n",
    "                    full_year_df = pd.read_csv(f, sep=',', index_col=0, dtype=dtype, parse_dates=parse_dates) \n",
    "                else:\n",
    "                    full_year_df = pd.read_csv(f, sep=',', index_col=0, \n",
    "                                               usecols = selected_columns, dtype=dtype, parse_dates=parse_dates) \n",
    "            print('The number of rows read: ', full_year_df.shape[0])\n",
    "    return full_year_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_combined_years(year, dir_year_files, selected_columns = None):\n",
    "\n",
    "    df_all_years = pd.DataFrame()\n",
    "\n",
    "    for year in years:\n",
    "        df_one_year = read_one_year_from_bucket_merged_csv(year = year, \n",
    "                                                          dir_prefix = dir_year_files,\n",
    "                                                          selected_columns = selected_columns)\n",
    "        df_all_years = df_all_years.append(df_one_year)\n",
    "\n",
    "    return df_all_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_locally(df, dir_prefix, year, as_json= False):\n",
    "    \"\"\" Saves df as json or csv locally on server \"\"\"\n",
    "    if as_json:        \n",
    "        file_path = dir_prefix + '/' + year + '_merged.json'\n",
    "        df.to_json(file_path)\n",
    "    else:\n",
    "        file_path =  dir_prefix + '/' + year + '_merged.csv'\n",
    "        df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading relocation dates\n",
    "blob_list = list(bucket.list_blobs(prefix='01_input/location_start_date.CSV'))\n",
    "\n",
    "for blob in blob_list: \n",
    "    with fs.open('graydon-data/' + blob.name) as f:\n",
    "        df_relocation_dates = pd.read_csv(f, sep=',', \n",
    "                                          na_values=['', '1198-06-12', 'NA']) \n",
    "        df_relocation_dates['date_relocation_last'] = pd.to_datetime(df_relocation_dates['date_relocation_last'])\n",
    "        df_relocation_dates['date_relocation_penultimate'] = pd.to_datetime(df_relocation_dates['date_relocation_penultimate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_company</th>\n",
       "      <th>id_branch</th>\n",
       "      <th>date_relocation_last</th>\n",
       "      <th>date_relocation_penultimate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>10079408</td>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>10079408</td>\n",
       "      <td>2015-06-11</td>\n",
       "      <td>2014-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>29898765</td>\n",
       "      <td>1998-02-21</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>10079424</td>\n",
       "      <td>2001-06-15</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>10079432</td>\n",
       "      <td>1997-03-20</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview of the data \n",
    "HTML(DataFrame(df_relocation_dates).head(5).to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_year = '2017'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows read:  22729762\n"
     ]
    }
   ],
   "source": [
    "df_year = read_one_year_from_bucket_merged_csv(year = i_year, dir_prefix = dir_year_files_from)\n",
    "qty_rows_input = len(df_year) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use only the relocations of i_year and the years before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_same_or_before_year = df_relocation_dates['date_relocation_last'].dt.year <= int(i_year)\n",
    "df_relocation_dates_year = df_relocation_dates[is_same_or_before_year]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding relocation dates for each branch month combination before the maximum date before the month date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_branch_months = df_year[['id_company', 'id_branch', 'date_month']]\n",
    "df_branch_months = df_branch_months.merge(df_relocation_dates_year, \n",
    "                                          on=['id_company', 'id_branch'], \n",
    "                                          how='left')\n",
    "df_max_dates = df_branch_months.groupby(['id_company', 'id_branch', 'date_month'])['date_relocation_last', 'date_relocation_penultimate'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the new data to the original year data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_year = df_year.merge(df_max_dates,\n",
    "                        on=['id_company', 'id_branch', 'date_month'], \n",
    "                        how='left')\n",
    "qty_rows_output = len(df_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether number of rows yearly should remain constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(qty_rows_input != qty_rows_output):\n",
    "    print('Mismatch in rows for ', i_year)\n",
    "    print(\"Going in : \", qty_rows_input)\n",
    "    print(\"Going out: \", qty_rows_output)\n",
    "    raise Exception('Mismatch in rows for: {}'.format(i_year))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e83b35bcfd68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_df_locally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_prefix\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdir_year_files_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mi_year\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-d43bdf9e9d51>\u001b[0m in \u001b[0;36msave_df_locally\u001b[0;34m(df, dir_prefix, year, as_json)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdir_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_merged.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1745\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    298\u001b[0m                                   \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                                   \u001b[0mdate_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                                   quoting=self.quoting)\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mrodriguezlara/graydon/graydon-moving/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mto_native_types\u001b[0;34m(self, slicer, na_rep, date_format, quoting, **kwargs)\u001b[0m\n\u001b[1;32m   2754\u001b[0m         result = tslib.format_array_from_datetime(\n\u001b[1;32m   2755\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2756\u001b[0;31m             format=format, na_rep=na_rep).reshape(values.shape)\n\u001b[0m\u001b[1;32m   2757\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.format_array_from_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/timestamps.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.timestamps.Timestamp.__new__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/conversion.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.conversion.convert_to_tsobject\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_df_locally(df= df_year, dir_prefix= dir_year_files_to, year= i_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date_month',\n",
       " 'id_company',\n",
       " 'id_branch',\n",
       " 'date_established',\n",
       " 'is_discontinued',\n",
       " 'code_discontinuation',\n",
       " 'code_financial_calamity',\n",
       " 'date_financial_calamity_started',\n",
       " 'date_financial_calamity_stopped',\n",
       " 'financial_calamity_outcome',\n",
       " 'code_legal_form',\n",
       " 'qty_employees',\n",
       " 'year_qty_employees',\n",
       " 'id_company_creditproxy',\n",
       " 'score_payment_assessment',\n",
       " 'amt_revenue',\n",
       " 'year_revenue',\n",
       " 'amt_operating_result',\n",
       " 'year_operating_result',\n",
       " 'amt_consolidated_revenue',\n",
       " 'year_consolidated_revenue',\n",
       " 'amt_consolidated_operating_result',\n",
       " 'year_consolidated_operating_result',\n",
       " 'qty_issued_credit_reports',\n",
       " 'perc_credit_limit_adjustment',\n",
       " 'color_credit_status',\n",
       " 'rat_pd',\n",
       " 'score_pd',\n",
       " 'has_increased_risk',\n",
       " 'is_sole_proprietor',\n",
       " 'code_sbi_2',\n",
       " 'code_sbi_1',\n",
       " 'qty_address_mutations_total',\n",
       " 'qty_address_mutations_month',\n",
       " 'date_start',\n",
       " 'from_date_start',\n",
       " 'has_relocated',\n",
       " 'qty_started_names',\n",
       " 'qty_stopped_names',\n",
       " 'has_name_change',\n",
       " 'total_changeof_board_members_',\n",
       " 'date_relocation_last',\n",
       " 'date_relocation_penultimate']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In loop form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i_year in years:\n",
    "    \n",
    "    # Reading year\n",
    "    print('1. Reading monthly branch data of ', i_year)\n",
    "    df_year = read_one_year_from_bucket_merged_csv(year = i_year, dir_prefix = dir_year_files_from)\n",
    "    qty_rows_input = len(df_year) \n",
    "    \n",
    "    # Use only the relocations of i_year and the years before\n",
    "    print('2. Selecting relocation data of ', i_year, \" and before\")\n",
    "    is_same_or_before_year = df_relocation_dates['date_relocation_last'].dt.year <= int(i_year)\n",
    "    df_relocation_dates_year = df_relocation_dates[is_same_or_before_year]\n",
    "    \n",
    "    # Getting relocation dates for each branch month combination before the maximum date before the month date\n",
    "    print('3. Getting relocation data for each month ', i_year, \" per branch\")\n",
    "    df_branch_months = df_year[['id_company', 'id_branch', 'date_month']]\n",
    "    df_branch_months = df_branch_months.merge(df_relocation_dates_year, \n",
    "                                              on=['id_company', 'id_branch'], \n",
    "                                              how='left')\n",
    "    df_max_dates = df_branch_months.groupby(['id_company', 'id_branch', 'date_month'])['date_relocation_last', 'date_relocation_penultimate'].max()\n",
    "    \n",
    "    # Adding the new data to the original year data\n",
    "    print('4. Adding relocation dates to company data of ', i_year)\n",
    "    df_year = df_year.merge(df_max_dates,\n",
    "                            on=['id_company', 'id_branch', 'date_month'], \n",
    "                            how='left')\n",
    "    qty_rows_output = len(df_year)\n",
    "    \n",
    "    # Check whether number of rows yearly should remain constant\n",
    "    if(qty_rows_input != qty_rows_output):\n",
    "        print('Mismatch in rows for ', i_year)\n",
    "        print(\"Going in : \", qty_rows_input)\n",
    "        print(\"Going out: \", qty_rows_output)\n",
    "        raise Exception('Mismatch in rows for: {}'.format(i_year))\n",
    "        \n",
    "    print('5. Saving enriched yearly data of ', i_year, ' locally' )       \n",
    "    save_df_locally(df= df_year, dir_prefix= dir_year_files_to, year= i_year)\n",
    "    \n",
    "    print('6. Done processing and saving', i_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graydon-moving",
   "language": "python",
   "name": "graydon-moving"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
