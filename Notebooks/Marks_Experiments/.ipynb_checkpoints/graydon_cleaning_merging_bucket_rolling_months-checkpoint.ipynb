{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ignore 'dask' warning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import gcsfs\n",
    "from google.cloud import storage\n",
    "from pandas import DataFrame\n",
    "from IPython.display import HTML\n",
    "from google.cloud.storage import Blob\n",
    "import datalab.storage as gcs_datalab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-set selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_dataset = datetime.strptime(\"2018-01-01\", '%Y-%m-%d')\n",
    "columns_dependent_vars = ['date_month', 'id_company', 'id_branch', 'is_sole_proprietor', 'has_relocated']\n",
    "columns_independent_vars = ['date_month', 'id_company', 'id_branch',\n",
    "                            'is_discontinued', 'financial_calamity_outcome', 'date_established', 'qty_employees', \n",
    "                            'year_qty_employees', 'id_company_creditproxy', 'score_payment_assessment', \n",
    "                            'amt_revenue', 'year_revenue', 'amt_consolidated_revenue', 'year_consolidated_revenue',\n",
    "                            'amt_consolidated_operating_result', 'year_consolidated_operating_result', \n",
    "                            'perc_credit_limit_adjustment', 'color_credit_status', 'rat_pd', 'score_pd',\n",
    "                            'has_increased_risk', 'is_sole_proprietor', 'code_SBI_2', 'code_SBI_1',\n",
    "                            'qty_address_mutations_total', 'qty_address_mutations_month', 'has_name_change', \n",
    "                            'code_discontinuation', 'code_financial_calamity', 'qty_issued_credit_reports', \n",
    "                            'Associate', 'Authorized official', 'Board member', 'Chairman', 'Commissioner', \n",
    "                            'Director', 'Liquidator', 'Major', 'Managing clerk', 'Managing partner', \n",
    "                            'Member of the partnership', 'Miscellaneous', 'Owner', 'Secretary', 'Secretary/Treasurer', \n",
    "                            'Treasurer', 'Unknown', 'Vice President', 'amt_operating_result', 'code_legal_form', \n",
    "                            'date_financial_calamity_started', 'date_financial_calamity_stopped', 'date_start', \n",
    "                            'from_date_start', 'qty_stopped_names', 'qty_started_names', 'year_operating_result']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up constants. All required\n",
    "project = 'graydon-moving-indicator'\n",
    "bucket_name = 'graydon-data'\n",
    "dir_data = 'including_scores/unzipped'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(project='graydon-moving-indicator')\n",
    "gcs = storage.Client()\n",
    "bucket = gcs.get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def month_delta(date, delta):\n",
    "    \"\"\" Adding/subtracting months to/from a date \"\"\"\n",
    "    m, y = (date.month + delta) % 12, date.year + ((date.month) + delta - 1) // 12\n",
    "    if not m: m = 12\n",
    "    d = min(date.day, [31, 29 if y%4==0 and not y%400==0 else 28,31,30,31,30,31,31,30,31,30,31][m-1])\n",
    "    return date.replace(day=d,month=m, year=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_board_members(df):\n",
    "    \"\"\"Agregates the number of board members into one feature \"\"\"    \n",
    "    col_list_to_sum = ['associate', 'authorized_official', 'board_member', 'chairman', 'commissioner',\n",
    "                       'director', 'liquidator', 'major', 'managing_clerk', 'managing_partner',\n",
    "                       'member_of_the_partnership', 'miscellaneous', 'owner', 'secretary',\n",
    "                       'secretary/treasurer', 'treasurer', 'unknown', 'vice_president']  \n",
    "    df['total_changeof_board_members_'] = df[col_list_to_sum].sum(axis=1)\n",
    "    df = df.drop(columns=col_list_to_sum)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_month_data(df):\n",
    "   \"\"\"Cleans data and returns formatted df\"\"\"\n",
    "   df['date_month'] = pd.to_datetime(df['date_month'])\n",
    "   df['financial_calamity_outcome'] = df['financial_calamity_outcome'].fillna(-1)\n",
    "   df['qty_employees'] = df['qty_employees'].str.strip()\n",
    "   df.loc[df.qty_employees == 'NA', 'qty_employees'] = None\n",
    "   df['qty_employees'] = df['qty_employees'].fillna(0)\n",
    "   df['qty_employees'] = df['qty_employees'].astype(str).astype(int)\n",
    "   df['year_qty_employees'] = df['year_qty_employees'].str.strip()\n",
    "   df.loc[df.year_qty_employees == 'NA', 'year_qty_employees'] = None\n",
    "   df['amt_revenue'] = df['amt_revenue'].str.strip()\n",
    "   df.loc[df.amt_revenue == 'NA', 'amt_revenue'] = np.NaN\n",
    "   df['amt_revenue'] = df['amt_revenue'].astype(str).str.replace(',','.').astype(float)\n",
    "   df['year_revenue'] = df['year_revenue'].str.strip()\n",
    "   df.loc[df.year_revenue == 'NA', 'year_revenue'] = None\n",
    "   df['amt_consolidated_revenue'] = df['amt_consolidated_revenue'].str.strip()\n",
    "   df.loc[df.amt_consolidated_revenue == 'NA', 'amt_consolidated_revenue'] = np.NaN\n",
    "   df['amt_consolidated_revenue'] = df['amt_consolidated_revenue'].astype(str).str.replace(',','.').astype(float)\n",
    "   df['year_consolidated_revenue'] = df['year_consolidated_revenue'].str.strip()\n",
    "   df.loc[df.year_consolidated_revenue == 'NA', 'year_consolidated_revenue'] = np.NaN\n",
    "   df['amt_consolidated_operating_result'] = df['amt_consolidated_operating_result'].str.strip()\n",
    "   df.loc[df.amt_consolidated_operating_result == 'NA', 'amt_consolidated_operating_result'] = np.NaN\n",
    "   df['amt_consolidated_operating_result'] = df['amt_consolidated_operating_result'].astype(str).str.replace(',','.').astype(float)\n",
    "   df['year_consolidated_operating_result'] = df['year_consolidated_operating_result'].str.strip()\n",
    "   df.loc[df.year_consolidated_operating_result == 'NA', 'year_consolidated_operating_result'] = np.NaN\n",
    "   df['score_pd'] = df['score_pd'].str.strip()\n",
    "   df.loc[df.score_pd == 'NA', 'score_pd'] = np.NaN\n",
    "   df['score_pd'] = df['score_pd'].astype(str).str.replace(',','.').astype(float)\n",
    "   df['has_increased_risk'] = df['has_increased_risk'].astype(bool)\n",
    "   df.loc[df.has_increased_risk == None, 'has_increased_risk'] = False\n",
    "   df.loc[df.code_sbi_2.isnull(), 'code_sbi_2'] = np.NaN\n",
    "   df.loc[df.date_established < '1700-12-31' , 'date_established'] = None\n",
    "   df['date_established'] = pd.to_datetime(df['date_established'])\n",
    "   df['amt_operating_result'] = df['amt_operating_result'].str.strip()\n",
    "   df.loc[df.amt_operating_result == 'NA', 'amt_operating_result'] = np.NaN\n",
    "   df['amt_operating_result'] = df['amt_operating_result'].astype(str).str.replace(',','.').astype(float)\n",
    "   df['year_operating_result'] = df['year_consolidated_operating_result'].str.strip()\n",
    "   #df.loc[df.year_operating_result == 'NA', 'year_operating_result'] = 0\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_relocation_dates(date_dataset):\n",
    "    \"\"\" Reading  relocation data \"\"\"\n",
    "    # Reading relocation dates\n",
    "    blob_list = list(bucket.list_blobs(prefix='location_start_date.CSV'))\n",
    "\n",
    "    for blob in blob_list: \n",
    "        with fs.open('graydon-data/' + blob.name) as f:\n",
    "            df_relocation_dates = pd.read_csv(f, sep=',', \n",
    "                                              na_values=['', '1198-06-12', 'NA']) \n",
    "            df_relocation_dates['date_relocation_last'] = pd.to_datetime(df_relocation_dates['date_relocation_last'])\n",
    "            df_relocation_dates['date_relocation_penultimate'] = pd.to_datetime(df_relocation_dates['date_relocation_penultimate'])\n",
    "            \n",
    "    return(df_relocation_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_month_filenames(df_date_months, dir_data):\n",
    "    \"\"\" Get the file names of number of the months in the data frame \"\"\"\n",
    "    month_files = [] # List of month files\n",
    "    \n",
    "    df_date_months['year'] = df_date_months.date_month.dt.year\n",
    "    list_years = df_date_months['year'].unique()\n",
    "\n",
    "    # If there are multiple years, iterate through years  \n",
    "    for year in list_years:\n",
    "        # Get the year's data file names\n",
    "        dir_data_year = dir_data + '/' + str(year)\n",
    "        list_blob = list(bucket.list_blobs(prefix=dir_data_year))\n",
    "\n",
    "        # finding out which month files should be processed by looking which contain the first month date (YYYY-mm-01)\n",
    "        df_year_months = df_date_months[df_date_months['year'] == year]['date_month']\n",
    "        for blob in list_blob:\n",
    "            for month in df_year_months:\n",
    "                if (month.strftime(\"%Y-%m-%d\") in blob.name) & ('CSV' in blob.name):\n",
    "                    month_files.append(blob.name)\n",
    "                    \n",
    "    return(month_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(date_month, columns_features, dir_data):\n",
    "    \"\"\" Getting the dependent variable set \"\"\"\n",
    "    df_months_combined = pd.DataFrame()  # The data frame which will contain all independent variables\n",
    "    \n",
    "    # Get all months in range\n",
    "    df_date_months = pd.DataFrame(pd.date_range(date_month, periods=12, freq=\"M\").tolist(),\n",
    "                                  columns=['date_month'])\n",
    "    df_date_months['date_month'] = df_date_months['date_month'].values.astype('datetime64[M]') # First day of month\n",
    "\n",
    "    # Get the file names of all required month files\n",
    "    month_files = get_month_filenames(df_date_months, dir_data)\n",
    "                    \n",
    "    # Cleaning, transforming and combining month files                \n",
    "    for month_file in month_files:\n",
    "        with fs.open('graydon-data/' + month_file) as f:\n",
    "            df_month = pd.read_csv(f, sep=';', usecols= columns_features, index_col=False)   \n",
    "            df_month = df_month[(df_month['is_sole_proprietor'] == 0)] # & (one_month_df['is_discontinued'] == 0) \n",
    "            df_month.columns = (df_month.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', ''))\n",
    "            df_months_combined = df_months_combined.append(df_month)\n",
    "            print('The number of rows so far by adding ', month_file, \":\", df_months_combined.shape[0])\n",
    "     \n",
    "    df_months_combined['date_dataset'] = date_month\n",
    "    \n",
    "    # Aggregating data to year\n",
    "    df_months_combined = df_months_combined.groupby(['date_dataset', \n",
    "                                                      'id_company', \n",
    "                                                      'id_branch']).agg({'has_relocated': 'max', \n",
    "                                                                         'date_month': 'max'})\n",
    "    df_months_combined = df_months_combined.rename(index=str, columns={\"date_month\": \"date_month_last\"})\n",
    "    df_months_combined = df_months_combined.reset_index()\n",
    "    df_months_combined['date_dataset'] = pd.to_datetime(df_months_combined['date_dataset'])\n",
    "    df_months_combined['id_company'] = df_months_combined['id_company'].astype(int)\n",
    "    df_months_combined['id_branch'] = df_months_combined['id_branch'].astype(int)\n",
    "    \n",
    "    return(df_months_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_targets(date_month, columns_targets, dir_data):\n",
    "    \"\"\" Getting the independent variable set \"\"\"\n",
    "    df_months_combined = pd.DataFrame()  # The data frame which will contain all independent variables\n",
    "    month_files = []                     # List of month files in scope\n",
    "    \n",
    "    # Get all months\n",
    "    date_start = month_delta(date_month, -12)\n",
    "    df_date_months = pd.DataFrame(pd.date_range(date_start, periods=12, freq=\"M\").tolist(),\n",
    "                                  columns=['date_month'])\n",
    "    df_date_months['date_month'] = df_date_months['date_month'].values.astype('datetime64[M]') # First day of month\n",
    "    \n",
    "    # Get the file names of all required month files\n",
    "    month_files = get_month_filenames(df_date_months, dir_data)\n",
    "    \n",
    "    # Cleaning, transforming and combining month files    \n",
    "    for month_file in month_files:\n",
    "        with fs.open('graydon-data/' + month_file) as f:\n",
    "            df_month = pd.read_csv(f, sep=';', usecols= columns_targets, index_col=False)   \n",
    "            df_month = df_month[(df_month['is_sole_proprietor'] == 0)] \n",
    "            df_month.columns = (df_month.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', ''))\n",
    "            df_month = aggregate_board_members(df_month)\n",
    "            df_month = clean_month_data(df_month)\n",
    "            df_months_combined = df_months_combined.append(df_month)\n",
    "            print('The number of rows so far by adding', month_file, \":\", df_months_combined.shape[0])\n",
    "            \n",
    "    df_months_combined['date_dataset'] = date_month        \n",
    "    \n",
    "    return(df_months_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upload_df_to_gc_bucket(df, dir_prefix, year, as_json= False):\n",
    "    \"\"\" Uploads pandas DF to Gc bucket either as json or csv \"\"\"\n",
    "    if as_json:\n",
    "        df_json = df.to_json()\n",
    "        new_file_path = dir_prefix + '/' + year + '_merged.json'\n",
    "        gcs_datalab.Bucket(bucket_name).item(new_file_path).write_to(df_json,'text/json')\n",
    "    else:\n",
    "        df_csv = df.to_csv()\n",
    "        new_file_path = dir_prefix + '/' + year + '_merged.csv'\n",
    "        gcs_datalab.Bucket(bucket_name).item(new_file_path).write_to(df_csv,'text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_df_locally(df, dir_prefix, dataset_name, as_json= False):\n",
    "    \"\"\" Saves df as json or csv locally on server \"\"\"\n",
    "    if as_json:        \n",
    "        file_path = dir_prefix + '/dataset_' + dataset_name + '.json'\n",
    "        df.to_json(file_path)\n",
    "    else:\n",
    "        file_path =  dir_prefix + '/dataset_' + dataset_name + '.csv'\n",
    "        df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_features = get_features(date_dataset, columns_features, dir_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_targets = get_targets(date_dataset, columns_targets, dir_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total = df_features.merge(df_targets,\n",
    "                             on=['id_company', 'id_branch', 'date_dataset'],\n",
    "                             how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the complete data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_df_locally(df= df_total, dir_prefix= 'files_to_bucket', dataset_name = str(date_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent variable : relocation indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Relocated:\", sum(df_features.has_relocated), \"of\", len(df_features), \"branches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the companies that 'died' within the dependent window the date_month_last value is before _yyyy-12-01_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features.groupby([date_month_last]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of records per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_targets.groupby(['date_month']).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graydon-moving",
   "language": "python",
   "name": "graydon-moving"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
